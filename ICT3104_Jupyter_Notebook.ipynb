{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a3aebdb",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "<a id='TOP'></a>\n",
    "# HOI Hub\n",
    "Welcome to the SIT ICT3104 Team 5 Human Object Interaction Hub Juypter Notebook!\n",
    "<br> This notebook will take you through the steps of running an \"out-of-the-box\" object detection model on video data.\n",
    "\n",
    "Notebook is currently preloaded with these models:\n",
    "1. [Toyota Smarthome (TSU) project](https://project.inria.fr/toyotasmarthome/)\n",
    "2. [I3D Feature Extraction](https://github.com/Finspire13/pytorch-i3d-feature-extraction)\n",
    "3. [TSU evaluation](https://github.com/dairui01/TSU_evaluation)\n",
    "4. [Nvidia STEP: Spatio-Temporal Progressive Learning for Video Action Detection](https://github.com/NVlabs/STEP)\n",
    "5. [MS-TCT: Multi-Scale Temporal ConvTransformer for Action Detection](https://github.com/dairui01/MS-TCT)\n",
    "6. [I3D models trained on Kinetics](https://github.com/piergiaj/pytorch-i3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import libraries\n",
    "Description: Starting with the base imports. This cell is used for importing relevant libraries for the project."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae75fb79",
   "metadata": {
    "hide_input": false,
    "scrolled": true,
    "tags": [
     "trial"
    ]
   },
   "outputs": [],
   "source": [
    "# Upload File using ipyfilechooser library\n",
    "from ipyfilechooser import FileChooser\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from tqdm.notebook import tqdm, trange\n",
    "# Video Player\n",
    "from IPython.display import Video, display, clear_output\n",
    "import time \n",
    "# Get the root directory of the project\n",
    "from pyprojroot import here\n",
    "# Copy File\n",
    "import shutil\n",
    "# Widget Packages\n",
    "import ipywidgets as widgets\n",
    "# In case widget extension not working\n",
    "# jupyter nbextension enable --py widge|tsnbextension\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv()\n",
    "os.environ['WANDB_API_KEY'] = os.getenv('WANDB_API_KEY')\n",
    "global_path = here(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7cb0ea",
   "metadata": {},
   "source": [
    "### Load Video Input Functions\n",
    "Description: This cell is used for opening the file selector for user to select the video as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7459f1aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def videoselectorinput():\n",
    "    starting_directory = '../data/video'\n",
    "    chooser = FileChooser(starting_directory)\n",
    "    display(chooser)\n",
    "    return chooser\n",
    "def videoselectoroutputTSU():\n",
    "    starting_directory = './video/output'\n",
    "    chooser = FileChooser(starting_directory)\n",
    "    display(chooser)\n",
    "    return chooser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd71b6b",
   "metadata": {},
   "source": [
    "## Pipeline Selection\n",
    "In this section, user can select the pipelines specified in this list to run the project:\n",
    "\n",
    "- Toyota TSU (Toyota Smarthome Untrimmed)\n",
    "- Nvidia STEP (Spatio-Temporal Progressive Learning for Video Action Detection)\n",
    "- MS-TCT (Multi-Scale Temporal ConvTransformer for Action Detection)\n",
    "\n",
    "Users can also add new pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adding new pipeline\n",
    "To add a new pipeline, add in the name of the pipeline in the options list in the dropdown widgets and add in the code for the new pipeline in a folder at the root directory.\n",
    "\n",
    "Description: This cell is used for setting and opening up the dropdown for the user to select the pipeline. After selecting and clicking \"Confirm\" the code will then run the selected pipeline."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205ee99d",
   "metadata": {
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(os.getcwd())\n",
    "pipelineList = ['TSU', 'STEP' , 'MS-TCT']\n",
    "pipelineSelectedMessage = widgets.Label()\n",
    "loadedDependenciesMessage = widgets.Label()\n",
    "pipelineDropdown = widgets.Dropdown(\n",
    "    options=pipelineList,\n",
    "    value=pipelineList[0],\n",
    "    description='Pipeline:')\n",
    "# Function on what happen when confirm is been click.\n",
    "def selectPipelineSet(b):\n",
    "    if pipelineDropdown.value == \"STEP\":\n",
    "        directory = os.getcwd()\n",
    "        if re.search(r\"TSU\", directory):\n",
    "            os.chdir('../')\n",
    "            path =here(\"./NVIDIA-STEP-MODEL/STEP\")\n",
    "            os.chdir(path)\n",
    "        elif re.search(r\"MSTCT\", directory):\n",
    "            os.chdir('../')\n",
    "            path =here(\"./NVIDIA-STEP-MODEL/STEP\")\n",
    "            os.chdir(path)\n",
    "        elif re.search(r\"STEP\", directory):\n",
    "            pass\n",
    "        else:\n",
    "            path =here(\"./NVIDIA-STEP-MODEL/STEP\")\n",
    "            os.chdir(path)\n",
    "        pipelineSelectedMessage.value = \"Running \" + pipelineDropdown.value + \" pipeline\"\n",
    "        loadedDependenciesMessage.value = \"Loaded dependecies for \" + pipelineDropdown.value\n",
    "    elif pipelineDropdown.value == \"TSU\":\n",
    "        directory = os.getcwd()\n",
    "        if re.search(r\"TSU\", directory):\n",
    "            pass\n",
    "        elif re.search(r\"STEP\", directory):\n",
    "            os.chdir('../../')\n",
    "            path =here(\"./pipeline\")\n",
    "            os.chdir(path)\n",
    "        elif re.search(r\"MSTCT\", directory):\n",
    "            os.chdir('../')\n",
    "            path =here(\"./pipeline\")\n",
    "            os.chdir(path)\n",
    "        else:\n",
    "            path =here(\"./pipeline\")\n",
    "            os.chdir(path)\n",
    "        pipelineSelectedMessage.value = \"Running \" + pipelineDropdown.value + \" pipeline\"\n",
    "        loadedDependenciesMessage.value = \"Loaded dependecies for \" + pipelineDropdown.value\n",
    "    elif pipelineDropdown.value == \"MS-TCT\":\n",
    "        directory = os.getcwd()\n",
    "        if re.search(r\"MSTCT\", directory):\n",
    "            pass\n",
    "        elif re.search(r\"STEP\", directory):\n",
    "            os.chdir('../../')\n",
    "            path =here(\"./MSTCT\")\n",
    "            os.chdir(path)\n",
    "        elif re.search(r\"TSU\", directory):\n",
    "            os.chdir('../')\n",
    "            path =here(\"./MSTCT\")\n",
    "            os.chdir(path)\n",
    "        else:\n",
    "            path =here(\"./MSTCT\")\n",
    "            os.chdir(path)\n",
    "        pipelineSelectedMessage.value = \"Running \" + pipelineDropdown.value + \" pipeline\"\n",
    "        loadedDependenciesMessage.value = \"Loaded dependecies for \" + pipelineDropdown.value\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "pipelineConfirm = widgets.Button(\n",
    "    description='Confirm',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "pipelineConfirm.on_click(selectPipelineSet)\n",
    "messagesBox =  widgets.VBox([loadedDependenciesMessage, pipelineSelectedMessage])\n",
    "pipelineBox = widgets.VBox([widgets.HBox([pipelineDropdown, pipelineConfirm]), messagesBox])\n",
    "pipelineBox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b17f71",
   "metadata": {},
   "source": [
    "<a href=\"#TSU\">Click here to go to Toyota TSU section</a><br>\n",
    "<a href=\"#STEP\">Click here to go to Nvidia STEP section</a><br>\n",
    "<a href='#MS-TCT'>Click here to go to MS-TCT section</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Toyota Smarthome (TSU)\n",
    "This section is based on pretrained model from [Toyota Smarthome Project](https://github.com/dairui01/Toyota_Smarthome).\n",
    "<br> With data sourced from [Toyota Smarthome (TSU) project](https://project.inria.fr/toyotasmarthome/)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "b68b99c7",
   "metadata": {},
   "source": [
    "## Data Exploration Section\n",
    "This section in the notebook that can load and display video data from the Toyota Smarthome (TSU) project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed518c9e",
   "metadata": {},
   "source": [
    "### Video Upload / Choose using ipyfilechooser\n",
    "Description: This cell is used for setting up the file choosers to select input video and to save output video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9251f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "video = videoselectorinput()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe9761f",
   "metadata": {},
   "source": [
    "### Upload selected video to the data folder (If needed)\n",
    "Description: This cell is used for uploading the video to data folder if video isn't already inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cdc476",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Upload Function\n",
    "# from pyprojroot import here\n",
    "# import shutil\n",
    "def upload(video):\n",
    "    print(video.selected)\n",
    "    source = video.selected\n",
    "    # Source path\n",
    "    # Destination path\n",
    "    destination = (here(\"./data/video\"))\n",
    "\n",
    "    # Copy file from the selected path\n",
    "    try:\n",
    "        shutil.copy(source, destination)\n",
    "        print(\"File copied successfully.\")\n",
    "\n",
    "    # If source and destination are same\n",
    "    except shutil.SameFileError:\n",
    "        print(\"Source and destination represents the same file.\")\n",
    "\n",
    "    # If destination is a directory.\n",
    "    except IsADirectoryError:\n",
    "        print(\"Destination is a directory.\")\n",
    "\n",
    "    # If there is any permission issue\n",
    "    except PermissionError:\n",
    "        print(\"Permission denied.\")\n",
    "\n",
    "    # For other errors\n",
    "    except:\n",
    "        print(\"Error occurred while copying file.\")\n",
    "\n",
    "upload(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd190ad",
   "metadata": {},
   "source": [
    "### Video Input Selection (Load video)\n",
    "Description: This cell is used for opening the file selector for user to select video input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ce6882",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select Video\n",
    "video = videoselectorinput()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73380e7d",
   "metadata": {},
   "source": [
    "### Video Player (Play selected video)\n",
    "Description: This cell is used for running the input video. This is for the user to check if it is the same video they selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09127163",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_path = video.selected\n",
    "Video(full_path, embed=True, width=540, html_attributes=\"controls muted autoplay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51191d83",
   "metadata": {},
   "source": [
    "<a id='TSU'></a>\n",
    "## Inference Section\n",
    "This section in the notebook that can perform inference using a pretrained HOI ML model based on the TSU project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90b56cd",
   "metadata": {},
   "source": [
    "### Load a pretrain model\n",
    "Description: This cell is used for setting up the dropdown list to allow the user to select the model to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263f4135",
   "metadata": {
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select model\n",
    "modelList = [] \n",
    "modelSelectedMessage = widgets.Label()\n",
    "\n",
    "# Select from the list of model in the pipeline/models folder\n",
    "for x in os.listdir(\"./models\"): \n",
    "    if os.path.isdir(os.path.join(\"./models\", x)) == False:\n",
    "        if (x.endswith(\".pkl\")) == False:\n",
    "            modelList.append(x)\n",
    "\n",
    "# Widgets\n",
    "confirmButton = widgets.Button(\n",
    "    description='Confirm',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    "    icon='check'\n",
    ")\n",
    "modelDropdown = widgets.Dropdown(\n",
    "    options=modelList,\n",
    "    value=modelList[0],\n",
    "    description='Model:')\n",
    "# Function on what happen when confirm is been click.\n",
    "def selectWidgetSet(b):\n",
    "    modelSelectedMessage.value = \"Selected: \" + modelDropdown.value\n",
    "    #print(\"Selected: \" , modelDropdown.value)\n",
    "\n",
    "confirmButton.on_click(selectWidgetSet)\n",
    "modelBox = widgets.VBox([widgets.HBox([modelDropdown, confirmButton]), modelSelectedMessage])\n",
    "modelBox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cad9e82",
   "metadata": {},
   "source": [
    "### Choose Input video to load into TSU model\n",
    "Description: This cell is used for selecting the video input to be passed to the TSU model.\n",
    "<br> Note: You only can select video that is in **testing subset** on the smarthome_CS_51.json file to run the inference video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a7e1db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select Video\n",
    "video = videoselectorinput()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8854da",
   "metadata": {},
   "source": [
    "### Run inference on the TSU model\n",
    "Description: This cell is used for running the inference.py script based on the video and model selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b5f2d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = modelDropdown.value\n",
    "loadmodel = './models/' + model\n",
    "videoPath = video.selected\n",
    "videoFile = video.selected_filename\n",
    "print(videoFile)\n",
    "print(videoPath)\n",
    "%run -it inference.py  -input_video_file $videoFile -model $model  -load_model $loadmodel -video_path $videoPath\n",
    "%wandb ict3104-team-5/inference-visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d326ce9c",
   "metadata": {},
   "source": [
    "### Output Video to view the inference result\n",
    "Description: This cell is used for print the filepath of the output video. Play the output video below the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afb7c95",
   "metadata": {
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "videoFileName = video.selected_filename[:-4]\n",
    "full_path = (os.path.join(here(\"./pipeline/video/output/\"),f\"{videoFileName}_caption.mp4\"))\n",
    "print(full_path)\n",
    "Video(full_path, embed=True, width=540, html_attributes=\"controls muted autoplay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6935d29c",
   "metadata": {},
   "source": [
    "## Feature Extraction Section\n",
    "This section in the notebook generates feature files from videos and validates the smarthome_CS_51.json to be used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87c12f9",
   "metadata": {},
   "source": [
    "### Run main feature-extraction function\n",
    "Description: This cell is used for running the main feature-extraction function to extract features from videos listed in video_paths.txt to create RGB .npy files for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99b994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(global_path , 'i3d-feature-extraction') \n",
    "%cd $path\n",
    "%run -it main.py feature_type=i3d device=\"cuda:0\" on_extraction=save_numpy streams=rgb output_path=../data/dataset/v_iashin_i3d/ file_with_video_paths=./sample/sample_video_paths.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c593e13",
   "metadata": {},
   "source": [
    "### Remove entries that do not have RGB .npy files\n",
    "Description: This cell is used for running validate_train_test.py to  remove entries that do not have RGB .npy files generated in a specific directory from smarthome_CS_51.json to create an updated one called smarthome_CS_51_v2.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba5d4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = FileChooser(f'{global_path}/data/dataset') \n",
    "fc.show_only_dirs = True\n",
    "fc.title = 'Select your RGB .npy files directory'\n",
    "\n",
    "def update_my_fname():\n",
    "    global my_fname\n",
    "    my_fname = fc.selected\n",
    "    return  \n",
    "\n",
    "fc.register_callback(update_my_fname)\n",
    "\n",
    "ratio_list = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "        \n",
    "def dropdown_eventhandler(change):\n",
    "    determine(training_ratio_dropdown.value)\n",
    "\n",
    "def determine(ratio):\n",
    "    testing_ratio_dropdown.value = 100 - ratio\n",
    "\n",
    "training_ratio_dropdown = widgets.Dropdown(description=\"Training ratio:\", options=ratio_list)\n",
    "testing_ratio_dropdown = widgets.Dropdown(description=\"Testing ratio:\", options=ratio_list, disabled=True)\n",
    "\n",
    "training_ratio_dropdown.observe(dropdown_eventhandler, names='value')\n",
    "\n",
    "button = widgets.Button(\n",
    "    description='Run',\n",
    "    disabled=False,\n",
    "    button_style='success'\n",
    ")\n",
    "\n",
    "display(fc, training_ratio_dropdown, testing_ratio_dropdown, button)\n",
    "\n",
    "def run_validation(b):\n",
    "    relative_path = os.path.relpath(my_fname, path)\n",
    "    \n",
    "    training_ratio = training_ratio_dropdown.value\n",
    "    testing_ratio = testing_ratio_dropdown.value\n",
    "    \n",
    "    print(f\"RGB .npy files directory: {relative_path}\")\n",
    "    print(f\"Training ratio: {training_ratio}\")\n",
    "    print(f\"Testing ratio: {testing_ratio}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    !python validate_train_test.py -rgb_path $relative_path -training_ratio $training_ratio -testing_ratio $testing_ratio\n",
    "    \n",
    "button.on_click(run_validation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67463a88",
   "metadata": {},
   "source": [
    "## Training Section\n",
    "This section in the notebook that can train a HOI ML model based on the TSU project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e20405",
   "metadata": {},
   "source": [
    "### Run model training function\n",
    "Description: This cell is used for form to fill up the parameters needed to run the run_PDAN.sh for training. Running the run_PDAN.sh script based on the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prints working Directory\n",
    "%pwd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d10fe28",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path = os.path.join(global_path , 'pipeline') \n",
    "os.chdir(path)\n",
    "title = widgets.HTML(\"<h1 style=\\\"text-align: center; \\\"><b>Run_PDAN</b></h1>\")\n",
    "label = widgets.HTML(\n",
    "    value=\"<b>Label</b>\\\n",
    "    <br><b>Dataset Input:</b> Name of the Dataset to train\\\n",
    "    <br><b>Mode:</b> Inference Metric Methods: rgb or flow\\\n",
    "    <br><b>Spilt Setting:</b> Either CS or CV\\\n",
    "    <br><b>Model:</b> Select Model To Train\\\n",
    "    <br><b>Train:</b> Train or Test\\\n",
    "    <br><b>Num Channel:</b> Number of channel for image processing\\\n",
    "    <br><b>Learning Rate:</b> Learning rate indicates the step size that gradient descent takes towards local optima\\\n",
    "    <br><b>Kernel Size:</b> Size of a convolutional filter\\\n",
    "    <br><b>AP Type:</b> Metric in measuring the accuracy of object detectors: map or wap\\\n",
    "    <br><b>Epoch:</b> Number of iterations for the training process in Machine Learning\\\n",
    "    <br><b>Batch_Size:</b> Number of samples that is used in one epoch.\\\n",
    "    <br><b>Compute Info:</b> Information of the computation\"\n",
    ")\n",
    "style = {'description_width': '90px'}\n",
    "\n",
    "# Using Dropdown method\n",
    "dataset_list = []\n",
    "directoryDataset = os.path.join(global_path , 'data' , 'dataset') # Directory of the dataset (NPY files)\n",
    "# Store the folder in the  dataset into dataset list\n",
    "for x in os.listdir(directoryDataset):\n",
    "    if os.path.isdir(os.path.join(directoryDataset, x)):\n",
    "        # print(os.path.join(directoryDataset, x))\n",
    "        dataset_list.append(x)\n",
    "\n",
    "datasetDropDown = widgets.Dropdown(\n",
    "    options=dataset_list,\n",
    "    value=dataset_list[0],\n",
    "    style=style,\n",
    "    description='Dataset:')\n",
    "\n",
    "\n",
    "mode_input = widgets.Text(\n",
    "    value='rgb',\n",
    "    placeholder='Enter Mode',\n",
    "    description='Mode:',\n",
    "    disabled=False,\n",
    "    style=style,\n",
    "    tooltip='Inference Metric Methods: rgb or flow'\n",
    "    \n",
    ")\n",
    "\n",
    "split_input = widgets.Text(\n",
    "    value='CS',\n",
    "    placeholder='Enter Split Setting',\n",
    "    description='Split Setting:',\n",
    "    disabled=False,\n",
    "    style=style,\n",
    "    tooltip='Either CS or CV'\n",
    ")\n",
    "\n",
    "# Using Dropdown method\n",
    "model_list = []\n",
    "model_list.append(\"PDAN_TSU_RGB\")\n",
    "        \n",
    "model_input = widgets.Dropdown(\n",
    "    options=model_list,\n",
    "    value=model_list[0],\n",
    "    description='Model:',\n",
    "    style=style,\n",
    "    tooltip='Select Model to train.'\n",
    ")\n",
    "\n",
    "\n",
    "train_input = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description = 'Train',\n",
    "    disabled=False,\n",
    "    indent=True,\n",
    "    style=style,\n",
    "    tooltip='Train or Test'\n",
    ")\n",
    "\n",
    "num_channel_input =  widgets.BoundedIntText(\n",
    "    value=512,\n",
    "    min=1,\n",
    "    max=1000,\n",
    "    step=1,\n",
    "    description='Num Channel:',\n",
    "    disabled=False,\n",
    "    style=style,\n",
    "    tooltip='Number of channel for image processing'\n",
    ")\n",
    "\n",
    "lr_input = widgets.FloatText(\n",
    "    value=0.0002,\n",
    "    description='Learning Rate:',\n",
    "    disabled=False,\n",
    "    style=style,\n",
    "    tooltip='Learning rate indicates the step size that gradient descent takes towards local optima'\n",
    ")\n",
    "\n",
    "kernel_size_input =  widgets.BoundedIntText(\n",
    "    value=3,\n",
    "    min=1,\n",
    "    max=5,\n",
    "    step=1,\n",
    "    description='Kernel Size:',\n",
    "    disabled=False,\n",
    "    style=style,\n",
    "    tooltip='Size of a convolutional filter'\n",
    ")\n",
    "\n",
    "aptype_input = widgets.Text(\n",
    "    value='map',\n",
    "    placeholder='Enter APType',\n",
    "    description='APType:',\n",
    "    disabled=False,\n",
    "    style=style,\n",
    "    tooltip='Metric in measuring the accuracy of object detectors: map or wap'\n",
    ")\n",
    "\n",
    "epoch_input = widgets.BoundedIntText(\n",
    "    value=140,\n",
    "    min=1,\n",
    "    max=1000,\n",
    "    step=1,\n",
    "    description='Epoch:',\n",
    "    disabled=False,\n",
    "    style=style,\n",
    "    tooltip='Number of iterations for the training process in Machine Learning'\n",
    ")\n",
    "\n",
    "batch_size_input = widgets.Dropdown(\n",
    "    options=['1', '2', '4', '8', '16', '32', '64', '128', '256', '512', '1024'],\n",
    "    value='1',\n",
    "    description='Batch_Size:',\n",
    "    disabled=False,\n",
    "    tooltip='Number of samples that is used in one epoch.',\n",
    "    style=style\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "comp_info_input = widgets.Text(\n",
    "    value='TSU_CS_RGB_PDAN',\n",
    "    placeholder='Enter Compute Info',\n",
    "    description='Compute Info:',\n",
    "    disabled=False,\n",
    "    style=style,\n",
    "    tooltip='Information of the computation.'\n",
    ")\n",
    "\n",
    "button = widgets.Button(\n",
    "    description='Save',\n",
    "    disabled=False,\n",
    "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Save Pref',\n",
    "    icon='check', # (FontAwesome names without the `fa-` prefix)\n",
    "    style=style\n",
    ")\n",
    "\n",
    "jsonDirectory = os.path.join(directoryDataset , 'JSON')\n",
    "jsonFiles = []\n",
    "for x in os.listdir(jsonDirectory):\n",
    "    if os.path.isdir(os.path.join(jsonDirectory, x)) == False:\n",
    "        jsonFiles.append(x)\n",
    "\n",
    "json_file_input = widgets.Dropdown(\n",
    "    options=jsonFiles,\n",
    "    value=jsonFiles[0],\n",
    "    style=style,\n",
    "    description='Json File:')\n",
    "    \n",
    "\n",
    "def selectWidgetSet(b):\n",
    "    print(\"Selected Dataset: \" , datasetDropDown.value)\n",
    "    print(\"Selected Mode: \" , mode_input.value)\n",
    "    print(\"Selected Split Setting: \" , split_input.value)\n",
    "    print(\"Selected Model: \" , model_input.value)\n",
    "    print(\"Selected Train: \" , train_input.value)\n",
    "    print(\"Selected Num Channel: \" , num_channel_input.value)\n",
    "    print(\"Selected Learning Rate: \" , lr_input.value)\n",
    "    print(\"Selected Kernel Size: \" , kernel_size_input.value)\n",
    "    print(\"Selected APType: \", aptype_input.value)\n",
    "    print(\"Selected Epoch: \", epoch_input.value)\n",
    "    print(\"Selected Batch: \" , batch_size_input.value)\n",
    "    print(\"Selected Compute Info: \" , comp_info_input.value)\n",
    "    dataset_path = os.path.join(\"../data/dataset/\" , datasetDropDown.value)\n",
    "    print(\"Dataset Path: \", dataset_path)\n",
    "    json_path = os.path.join(\"../data/dataset/JSON/\" , json_file_input.value)\n",
    "    print(\"Json Path: \", json_path)\n",
    "    %run -i train.py -dataset $datasetDropDown.value -mode \\\n",
    "    $mode_input.value -split_setting $split_input.value -model $model_input.value \\\n",
    "    -train $train_input.value -num_channel $num_channel_input.value -lr $lr_input.value \\\n",
    "    -kernelsize $kernel_size_input.value -APtype $aptype_input.value \\\n",
    "    -epoch $epoch_input.value -batch_size $batch_size_input.value -comp_info $comp_info_input.value \\\n",
    "    -rgb_root $dataset_path -json_file $json_path\n",
    "    %wandb ict3104-team-5/training-visualisation\n",
    "\n",
    "display(title, label, datasetDropDown, mode_input,split_input, model_input, train_input, num_channel_input, lr_input, kernel_size_input, aptype_input, epoch_input, batch_size_input, comp_info_input, json_file_input ,button)\n",
    "button.on_click(selectWidgetSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba99aa7",
   "metadata": {},
   "source": [
    "## Testing Section\n",
    "This section in the notebook that will evaluate a trained model based on the TSU project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b674a8",
   "metadata": {},
   "source": [
    "### Model selection for Testing\n",
    "Description: This cell is used for dropdown for the user to select the model to be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6ef6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(global_path , 'pipeline') \n",
    "os.chdir(path)\n",
    "# Select model\n",
    "modelList = []\n",
    "modelSelectedTestingMessage = widgets.Label() \n",
    "\n",
    "# Select from the list of model in the pipeline/models folder\n",
    "for x in os.listdir(\"./models\"): \n",
    "    if os.path.isdir(os.path.join(\"./models\", x)) == False:\n",
    "        if (x.endswith(\".pkl\")):\n",
    "            modelList.append(x)\n",
    "\n",
    "# Widgets\n",
    "confirmButton = widgets.Button(\n",
    "    description='Confirm',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    "    icon='check'\n",
    ")\n",
    "modelDropdown = widgets.Dropdown(\n",
    "    options=modelList,\n",
    "    value=modelList[0],\n",
    "    description='Model:')\n",
    "# Function on what happen when confirm is been click.\n",
    "def selectWidgetSet(b):\n",
    "    modelSelectedTestingMessage.value = \"Selected: \" + modelDropdown.value\n",
    "    %run -i test.py -split CS -pkl_path \"models/\"$modelDropdown.value\n",
    "    %wandb ict3104-team-5/testing-visualisation\n",
    "\n",
    "confirmButton.on_click(selectWidgetSet)\n",
    "modelBox = widgets.VBox([widgets.HBox([modelDropdown, confirmButton]), modelSelectedTestingMessage])\n",
    "modelBox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc42e8ee",
   "metadata": {},
   "source": [
    "### Inference Section for Training model\n",
    "Description: This cell runs inference on the training model selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217f2136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model\n",
    "modelList = [] \n",
    "\n",
    "# Select from the list of model in the pipeline/models folder\n",
    "for x in os.listdir(\"./models\"): \n",
    "    if os.path.isdir(os.path.join(\"./models\", x)) == False:\n",
    "        if (x.endswith(\".pkl\")) == False:\n",
    "            modelList.append(x)\n",
    "\n",
    "# Widgets\n",
    "confirmButton = widgets.Button(\n",
    "    description='Confirm',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    "    icon='check'\n",
    ")\n",
    "modelDropdown = widgets.Dropdown(\n",
    "    options=modelList,\n",
    "    value=modelList[0],\n",
    "    description='Model:')\n",
    "# Function on what happen when confirm is been click.\n",
    "def selectWidgetSet(b):\n",
    "    print(\"Selected: \" , modelDropdown.value)\n",
    "    model = modelDropdown.value\n",
    "    loadmodel = './models/' + model\n",
    "    videoPath = video.selected\n",
    "    videoFile = video.selected_filename\n",
    "    print(videoFile)\n",
    "    print(videoPath)\n",
    "    %run -it inference.py  -input_video_file $videoFile -model $model  -load_model $loadmodel -video_path $videoPath\n",
    "    %wandb ict3104-team-5/inference-visualisation\n",
    "\n",
    "confirmButton.on_click(selectWidgetSet)\n",
    "modelBox = widgets.VBox([widgets.HBox([modelDropdown, confirmButton])])\n",
    "modelBox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3c5d1e",
   "metadata": {},
   "source": [
    "### Output Video for Inference Section for Training model\n",
    "Description: This cell runs inference on the training model selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9f100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "videoFileName = video.selected_filename[:-4]\n",
    "full_path = (os.path.join(here(\"./pipeline/video/output/\"),f\"{videoFileName}_caption.mp4\"))\n",
    "print(full_path)\n",
    "Video(full_path, embed=True, width=540, html_attributes=\"controls muted autoplay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "5143aac1",
   "metadata": {},
   "source": [
    "<a id='STEP'></a>\n",
    "# NVIDIA STEP Section\n",
    "## Nvidia Step Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566bc6f4",
   "metadata": {},
   "source": [
    "**Description:** This cell is used for setup the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdf4465",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Run this cell to setup the NVIDIA STEP.\n",
    "%run -i setup.py build develop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd4ec16",
   "metadata": {},
   "source": [
    "<a href=\"#INFERENCE\">Click here to go to NVIDIA STEP INFERENCE Section</a><br>\n",
    "<a href=\"#TRAINING\">Click here to go to NVIDIA STEP TRAINING Section</a><br>\n",
    "<a href=\"#TESTING\">Click here to go to NVIDIA STEP TESTING Section</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce93efc",
   "metadata": {},
   "source": [
    "<a id='INFERENCE'></a>\n",
    "## INFERENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0728041",
   "metadata": {},
   "source": [
    "**Description:** This cell is used for selection of the dataset and the video input to run INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18281ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Select video\n",
    "video_list = [] \n",
    "dataset_list = [] \n",
    "model_list = [] \n",
    "directoryVideo = \"./Input/\" # Directory of the video (.mp4)\n",
    "directoryDataset = \"./datasets/Inference/frames/\" # Directory of the dataset (NPY files)\n",
    "directoryModel = \"./pretrained/\" # Directory of the pretrained model\n",
    "\n",
    "datasetStepSelectedMessage = widgets.Label()\n",
    "videoStepSelectedMessage = widgets.Label()\n",
    "# Store the folder in the  dataset into dataset list\n",
    "for x in os.listdir(directoryDataset):\n",
    "    if os.path.isdir(os.path.join(directoryDataset, x)):\n",
    "        # print(os.path.join(directoryDataset, x))\n",
    "        dataset_list.append(x)\n",
    "\n",
    "# Store the video names into video list\n",
    "for x in os.listdir(directoryVideo):\n",
    "    if x.endswith(\".mp4\"):\n",
    "        video_list.append(x)\n",
    "        \n",
    "# Store the pretained model names into pretrained model list\n",
    "for x in os.listdir(directoryModel):\n",
    "    if x.endswith(\".pth\"):\n",
    "        model_list.append(x)\n",
    "        \n",
    "datasetDropDown = widgets.Dropdown(\n",
    "    options=dataset_list,\n",
    "    description='Dataset:')\n",
    "\n",
    "videoDropDown = widgets.Dropdown(\n",
    "    options=video_list,\n",
    "    description='Video: ')\n",
    "\n",
    "modelDropDown = widgets.Dropdown(\n",
    "    options=model_list,\n",
    "    description='Model: ')\n",
    "\n",
    "# Widgets\n",
    "confirmButton = widgets.Button(\n",
    "    description='Confirm',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    "    icon='check'\n",
    ")\n",
    "# Function on what happen when confirm is been click. To be intergrated with some other code\n",
    "def selectTrimSet(b):\n",
    "    datasetStepSelectedMessage.value = \"Selected Dataset: \" + datasetDropDown.value\n",
    "    videoStepSelectedMessage.value = \"Selected Video: \" + videoDropDown.value\n",
    "    #print(\"Selected Dataset: \" , datasetDropDown.value)\n",
    "    #print(\"Selected Video: \" , videoDropDown.value)\n",
    "    print(\"Selected model: \" , modelDropDown.value)\n",
    "\n",
    "confirmButton.on_click(selectTrimSet)\n",
    "display(datasetDropDown, videoDropDown, modelDropDown, confirmButton, datasetStepSelectedMessage, videoStepSelectedMessage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f1ba03",
   "metadata": {},
   "source": [
    "**Description:** This cell is used to caption every frame of the video. Afterwards, it will run the STEP Model.<br>\n",
    "**Note:** The root folder selected will be use in this section to store the frames generated from the video selected in the frames folder within the root folder.<br> *** Output frames result from the INFERENCE will be store in the Output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c0f48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "cap= cv2.VideoCapture('Input/' + videoDropDown.value)\n",
    "i=0\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret == False:\n",
    "        break\n",
    "    cv2.imwrite('datasets/Inference/frames/' + datasetDropDown.value + '/frame'+str(i).zfill(4)+'.jpg',frame)\n",
    "    print('frame generated: ' + str(i).zfill(4) + '.jpg')\n",
    "    i+=1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "data_root=\"datasets/Inference/\"\n",
    "%run -i demo.py -model $modelDropDown.value -root_folder $data_root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23d9f6d",
   "metadata": {},
   "source": [
    "**Description:** This cell is used for reading every frames in the results folder.It will write the captions of the frames onto the output video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9872d574",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "img_array = []\n",
    "for filename in glob.glob('Output/results/' + datasetDropDown.value + '/*.jpg'):\n",
    "    img = cv2.imread(filename) \n",
    "    height, width, layers = img.shape\n",
    "    size = (width,height)\n",
    "    img_array.append(img)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc('F', 'M', 'P', '4')\n",
    "out = cv2.VideoWriter('Output/'+videoDropDown.value, fourcc, 29, size)\n",
    " \n",
    "for i in range(len(img_array)):\n",
    "    out.write(img_array[i])\n",
    "out.release()\n",
    "\n",
    "vdoFileName = videoDropDown.value\n",
    "full_path = (os.path.join(here(\"./Output/\"),f\"{vdoFileName}\"))\n",
    "print(full_path)\n",
    "Video(full_path, embed=True, width=540, html_attributes=\"controls muted autoplay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4176e91",
   "metadata": {},
   "source": [
    "<a id='TRAINING'></a>\n",
    "## TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28604a44",
   "metadata": {},
   "source": [
    "**Description:** The cell will generate pickle files (.PKL) from the csv files prepared in \"datasets/ava/label/\"<br>\n",
    "**Please change the content in the csv files according to the name of the of the video you downloaded from AVA dataset** <br>\n",
    "**Note:** The current csv files, each contain one video name in the first column. You can look for more information of NVIDIA STEP <a href=\"https://github.com/NVlabs/STEP\">here</a>.<br>\n",
    "Download AVA dataset version 2.1 <a href=\"https://research.google.com/ava/download/ava_v2.1.zip\">here</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014eac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "%run ./scripts/generate_label.py \"datasets/ava/label/ava_train_v2.1.csv\" \n",
    "%run ./scripts/generate_label.py \"datasets/ava/label/ava_val_v2.1.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5c5933",
   "metadata": {},
   "source": [
    "**Description:** This cell will extract frames out from the video input <br>\n",
    "**To get the video for training using AVA dataset, please go to https://github.com/cvdfoundation/ava-dataset to download the video**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cba27c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./scripts/extract_clips.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48336e4",
   "metadata": {},
   "source": [
    "**Description:** Customize the setting to run the training. For device with low resources recommended to set **num_workers to 2 and batch_size to 1 or both at 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a30ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root=\"datasets/ava/\"\n",
    "save_root=\"datasets/ava/cache/\"\n",
    "pretrain_path=\"pretrained/ava_step.pth\"\n",
    "\n",
    "name=\"STEP\"\n",
    "base_net=\"i3d\"\n",
    "det_net=\"two_branch\"\n",
    "resume_path=\"Auto\"\n",
    "\n",
    "T=3\n",
    "max_iter=3    # index starts from 1\n",
    "iterative_mode=\"temporal\"\n",
    "anchor_mode=\"1\"\n",
    "temporal_mode=\"predict\"\n",
    "pool_mode=\"align\"\n",
    "pool_size=7\n",
    "\n",
    "# training schedule\n",
    "num_workers=2\n",
    "max_epochs=144\n",
    "batch_size=1\n",
    "optimizer=\"adam\"\n",
    "base_lr=7.5e-5\n",
    "det_lr0=1.5e-4\n",
    "det_lr=7.5e-4\n",
    "save_step=11465\n",
    "print_step=500\n",
    "scheduler=\"cosine\"\n",
    "milestones=\"-1\"\n",
    "warmup_iters=1000\n",
    "\n",
    "# losses\n",
    "dropout=0.3\n",
    "fc_dim=256\n",
    "lambda_reg=5\n",
    "lambda_neighbor=1\n",
    "cls_thresh=\"0.2,0.35,0.5\"\n",
    "reg_thresh=\"0.2,0.35,0.5\"\n",
    "max_pos_num=5\n",
    "neg_ratio=2\n",
    "NUM_SAMPLE=-1\n",
    "topk=300\n",
    "evaluate_topk=300\n",
    "\n",
    "# data augmentation / normalization\n",
    "scale_norm=2    # for i3d\n",
    "do_flip=\"True\"\n",
    "do_crop=\"True\"\n",
    "do_photometric=\"True\"\n",
    "do_erase=\"True\"\n",
    "freeze_affine=\"True\"\n",
    "freeze_stats=\"True\"\n",
    "\n",
    "\n",
    "%run train.py --data_root $data_root --save_root $save_root \\\n",
    "    --name $name --pretrain_path $pretrain_path --resume_path $resume_path \\\n",
    "    --base_net $base_net --det_net $det_net --max_iter $max_iter --T $T \\\n",
    "    --iterative_mode $iterative_mode --anchor_mode $anchor_mode --anchor_mode $anchor_mode --temporal_mode $temporal_mode \\\n",
    "    --pool_mode $pool_mode --pool_size $pool_size --save_step $save_step --topk $topk --evaluate_topk $evaluate_topk \\\n",
    "    --num_workers $num_workers --max_epochs $max_epochs --batch_size $batch_size --print_step $print_step \\\n",
    "    --optimizer $optimizer --base_lr $base_lr --det_lr $det_lr --det_lr0 $det_lr0 --milestones $milestones \\\n",
    "    --scale_norm $scale_norm --do_flip $do_flip --do_crop $do_crop --do_photometric $do_photometric --do_erase $do_erase \\\n",
    "    --fc_dim $fc_dim --dropout $dropout --NUM_SAMPLE $NUM_SAMPLE --scheduler $scheduler --warmup_iters $warmup_iters \\\n",
    "    --cls_thresh $cls_thresh --reg_thresh $reg_thresh --max_pos_num $max_pos_num --neg_ratio $neg_ratio \\\n",
    "    --freeze_affine $freeze_affine --freeze_stats $freeze_stats --lambda_reg $lambda_reg --lambda_neighbor $lambda_neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3204b22",
   "metadata": {},
   "source": [
    "**Description:** This cell will copy all the file with \".pth\" extention to \"./pretrained/\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5282ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "src_path = 'datasets/ava/cache/STEP-max3-i3d-two_branch/'\n",
    "trg_path = 'pretrained/'\n",
    "\n",
    "for src_file in Path(src_path).glob('*.pth'):\n",
    "    shutil.copy(src_file, trg_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a23aa5",
   "metadata": {},
   "source": [
    "<a id='TESTING'></a>\n",
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b7a415",
   "metadata": {},
   "source": [
    "**Description:** This cell is to run the testing for NVIDIA STEP model <br>\n",
    "Run the following command for testing and evaluation on the validation set of AVA <br>\n",
    "**Note:** The output will be stored in **\"datasets/ava/cache/STEP-max3-i3d-two_branch/\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04701e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model\n",
    "model_list = [] \n",
    "directoryModel = \"./pretrained/\" # Directory of the pretrained model\n",
    "\n",
    "# Store the pretained model names into pretrained model list\n",
    "for x in os.listdir(directoryModel):\n",
    "    if x.endswith(\".pth\"):\n",
    "        model_list.append(x)      \n",
    "\n",
    "modelDropDown = widgets.Dropdown(\n",
    "    options=model_list,\n",
    "    description='Model: ')\n",
    "\n",
    "# Widgets\n",
    "confirmButton = widgets.Button(\n",
    "    description='Confirm',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    "    icon='check'\n",
    ")\n",
    "# Function on what happen when confirm is been click. To be intergrated with some other code\n",
    "def selectTrimSet(b):\n",
    "    print(\"Selected model: \" , modelDropDown.value)\n",
    "\n",
    "confirmButton.on_click(selectTrimSet)\n",
    "print(\"Selected pretrained model for testing.\")\n",
    "display(modelDropDown, confirmButton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a343fb17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run test.py -model $modelDropDown.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e63e8e1",
   "metadata": {},
   "source": [
    "<a id='MS-TCT'></a>\n",
    "# MS-TCT Section\n",
    "T05-173: As a user, I want to set up the environment for MS-TCT, so that I am able to run the base code <br>\n",
    "<a href=\"#TOP\">Click here to go back to the import library section</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763da693",
   "metadata": {},
   "source": [
    "## PyTorch I3D Feature Extraction\n",
    "In this section we will be extracting the npy file for the I3D model. Select the dataset folder to extract the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50da3192",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = []\n",
    "directoryDataset = os.path.join(global_path , 'data' , 'dataset') # Directory of the dataset (NPY files)\n",
    "# Store the folder in the  dataset into dataset list\n",
    "for x in os.listdir(directoryDataset):\n",
    "    if os.path.isdir(os.path.join(directoryDataset, x)):\n",
    "        # print(os.path.join(directoryDataset, x))\n",
    "        dataset_list.append(x)\n",
    "datasetDropDown = widgets.Dropdown(\n",
    "    options=dataset_list,\n",
    "    value=dataset_list[0],\n",
    "    description='Dataset:')\n",
    "# Widgets\n",
    "confirmButton = widgets.Button(\n",
    "    description='Confirm',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    "    icon='check'\n",
    ")\n",
    "def selectDataset(b):\n",
    "    print(\"Selected dataset: \" + datasetDropDown.value)\n",
    "confirmButton.on_click(selectDataset)\n",
    "dataBox = widgets.VBox([widgets.HBox([datasetDropDown, confirmButton])])\n",
    "dataBox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8e47b2",
   "metadata": {},
   "source": [
    "Select folder to save the feature extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a75178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resultPyTorchI3D():\n",
    "    starting_directory = './data'\n",
    "    chooser = FileChooser(starting_directory)\n",
    "    chooser.show_only_dirs = True\n",
    "    display(chooser)\n",
    "    return chooser\n",
    "result = resultPyTorchI3D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15658dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../pytorch-i3d-master\n",
    "dataset_path  = os.path.join(\"../data/dataset/\" , datasetDropDown.value)\n",
    "print(\"Dataset Path: \", dataset_path)\n",
    "print(\"Result Path: \" , result.selected)\n",
    "result_path = result.selected\n",
    "start_path = os.getcwd()\n",
    "relative_path = os.path.relpath(result_path, start_path)\n",
    "# Convert \\\\ to / for relative path\n",
    "for i in range(len(relative_path)):\n",
    "    if relative_path[i] == '\\\\':\n",
    "        relative_path = relative_path[:i] + '/' + relative_path[i+1:]\n",
    "print(relative_path)\n",
    "%run -it extract_features.py -mode rgb -root ../data/dataset/Charades -gpu 0 -save_dir $relative_path -load_model ./models/rgb_charades.pt\n",
    "%cd ../MSTCT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2b8a47",
   "metadata": {},
   "source": [
    "## Training Section\n",
    "In this section we will be training the model using the extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcc6ea2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "title = widgets.HTML(\"<h1 style=\\\"text-align: center; \\\"><b>Run_MSTCT_Charades</b></h1>\")\n",
    "label = widgets.HTML(\n",
    "    value=\"<b>Label</b>\\\n",
    "    <br><b>Dataset Input:</b> Name of the Dataset to train\\\n",
    "    <br><b>Mode:</b> Inference Metric Methods: rgb or flow\\\n",
    "    <br><b>Model:</b> Select Model To Train\\\n",
    "    <br><b>Train:</b> Train or Test\\\n",
    "    <br><b>Num Clips:</b> Number of Clips for image processing\\\n",
    "    <br><b>Learning Rate:</b> Learning rate indicates the step size that gradient descent takes towards local optima\\\n",
    "    <br><b>Skip:</b> Skip\\\n",
    "    <br><b>Alpha Learning Rate:</b> Alpha Learning Rate\\\n",
    "    <br><b>Beta Learning Rate:</b> Beta Learning Rate\\\n",
    "    <br><b>Epoch:</b> Number of iterations for the training process in Machine Learning\\\n",
    "    <br><b>Batch_Size:</b> Number of samples that is used in one epoch.\\\n",
    "    <br><b>Compute Info:</b> Information of the computation \\\n",
    "    <br><b>Uni Size:</b> Uni Size\"\n",
    ")\n",
    "style = {'description_width': '90px'}\n",
    "\n",
    "# Using Dropdown method\n",
    "dataset_list = []\n",
    "directoryDataset = os.path.join(global_path , 'data' , 'dataset') # Directory of the dataset (NPY files)\n",
    "# Store the folder in the  dataset into dataset list\n",
    "for x in os.listdir(directoryDataset):\n",
    "    if os.path.isdir(os.path.join(directoryDataset, x)):\n",
    "        # print(os.path.join(directoryDataset, x))\n",
    "        dataset_list.append(x)\n",
    "\n",
    "datasetDropDown = widgets.Dropdown(\n",
    "    options=dataset_list,\n",
    "    value=dataset_list[0],\n",
    "    style=style,\n",
    "    description='Dataset:')\n",
    "\n",
    "\n",
    "mode_input = widgets.Text(\n",
    "    value='rgb',\n",
    "    placeholder='Enter Mode',\n",
    "    description='Mode:',\n",
    "    disabled=False,\n",
    "    style=style,\n",
    "    tooltip='Inference Metric Methods: rgb or flow'\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "# Using Dropdown method\n",
    "model_list = []\n",
    "model_list.append(\"MS_TCT\")\n",
    "        \n",
    "model_input = widgets.Dropdown(\n",
    "    options=model_list,\n",
    "    value=model_list[0],\n",
    "    description='Model:',\n",
    "    style=style,\n",
    "    tooltip='Select Model to train.'\n",
    ")\n",
    "\n",
    "train_input = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description = 'Train',\n",
    "    disabled=False,\n",
    "    indent=True,\n",
    "    style=style,\n",
    "    tooltip='Train or Test'\n",
    ")\n",
    "\n",
    "\n",
    "num_clips_input =  widgets.BoundedIntText(\n",
    "    value=256,\n",
    "    min=1,\n",
    "    max=1000,\n",
    "    step=1,\n",
    "    description='Num Clips:',\n",
    "    disabled=False,\n",
    "    style=style,\n",
    "    tooltip='Number of clips for image processing'\n",
    ")\n",
    "\n",
    "skip_input = widgets.BoundedIntText(\n",
    "    value=0,\n",
    "    placeholder='Enter Skip',\n",
    "    description='Skip Setting:',\n",
    "    disabled=False,\n",
    "    style=style,\n",
    "    tooltip='Skip Setting'\n",
    ")\n",
    "\n",
    "\n",
    "lr_input = widgets.FloatText(\n",
    "    value=0.0001,\n",
    "    description='Learning Rate:',\n",
    "    disabled=False,\n",
    "    style=style,\n",
    "    tooltip='Learning rate indicates the step size that gradient descent takes towards local optima'\n",
    ")\n",
    "\n",
    "\n",
    "comp_info_input = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description = 'Compute Info',\n",
    "    disabled=False,\n",
    "    indent=True,\n",
    "    style=style,\n",
    "    tooltip='True or False'\n",
    ")\n",
    "\n",
    "\n",
    "epoch_input = widgets.BoundedIntText(\n",
    "    value=50,\n",
    "    min=1,\n",
    "    max=1000,\n",
    "    step=1,\n",
    "    description='Epoch:',\n",
    "    disabled=False,\n",
    "    style=style,\n",
    "    tooltip='Number of iterations for the training process in Machine Learning'\n",
    ")\n",
    "\n",
    "\n",
    "unisize_input =  widgets.Checkbox(\n",
    "    value=True,\n",
    "    description = 'Unisize',\n",
    "    disabled=False,\n",
    "    indent=True,\n",
    "    style=style,\n",
    "    tooltip='True Or False'\n",
    ")\n",
    "\n",
    "alpha_l_input = widgets.FloatText(\n",
    "    value=1,\n",
    "    description='Alpha l:',\n",
    "    disabled=False,\n",
    "    style=style,\n",
    "    tooltip='Learning rate for Alpha'\n",
    ")\n",
    "\n",
    "beta_l_input = widgets.FloatText(\n",
    "    value=0.05,\n",
    "    description='Beta l:',\n",
    "    disabled=False,\n",
    "    style=style,\n",
    "    tooltip='Learning rate for Beta'\n",
    ")\n",
    "\n",
    "batch_size_input = widgets.Dropdown(\n",
    "    options=['1', '2', '4', '8', '16', '32', '64', '128', '256', '512', '1024'],\n",
    "    value='1',\n",
    "    description='Batch_Size:',\n",
    "    disabled=False,\n",
    "    tooltip='Number of samples that is used in one epoch.',\n",
    "    style=style\n",
    "    \n",
    ")\n",
    "\n",
    "button = widgets.Button(\n",
    "    description='Save',\n",
    "    disabled=False,\n",
    "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Save Pref',\n",
    "    icon='check', # (FontAwesome names without the `fa-` prefix)\n",
    "    style=style\n",
    ")\n",
    "\n",
    "jsonDirectory = os.path.join(directoryDataset , 'JSON')\n",
    "jsonFiles = []\n",
    "for x in os.listdir(jsonDirectory):\n",
    "    if os.path.isdir(os.path.join(jsonDirectory, x)) == False:\n",
    "        jsonFiles.append(x)\n",
    "\n",
    "json_file_input = widgets.Dropdown(\n",
    "    options=jsonFiles,\n",
    "    value=jsonFiles[0],\n",
    "    style=style,\n",
    "    description='Json File:')\n",
    "    \n",
    "\n",
    "def selectWidgetSet(b):\n",
    "    print(\"Selected Dataset: \" , datasetDropDown.value)\n",
    "    print(\"Selected Mode: \" , mode_input.value)\n",
    "    print(\"Selected Model: \" , model_input.value)\n",
    "    print(\"Selected Train: \" , train_input.value)\n",
    "    print(\"Selected Num Clips: \" , num_clips_input.value)\n",
    "    print(\"Selected Skip: \" , skip_input.value)\n",
    "    print(\"Selected Learning Rate: \" , lr_input.value)\n",
    "    print(\"Selected Compute Info: \" , comp_info_input.value)\n",
    "    print(\"Selected Epoch: \", epoch_input.value)\n",
    "    print(\"Selected Uni Size: \" , unisize_input.value)\n",
    "    print(\"Selected Alphal: \", alpha_l_input.value)\n",
    "    print(\"Selected Betal: \", beta_l_input.value)\n",
    "    print(\"Selected Batch: \" , batch_size_input.value)\n",
    "    dataset_path = os.path.join(\"./data/\" , datasetDropDown.value)\n",
    "    print(\"Dataset Path: \", dataset_path)\n",
    "    output_path = os.path.join(\"./save_logit/\")\n",
    "    # delete(output_path , '.pkl')\n",
    "    %run -it train.py -dataset $datasetDropDown.value \\\n",
    "    -mode $mode_input.value -model $model_input.value \\\n",
    "    -train $train_input.value -num_clips $num_clips_input.value -skip $skip_input.value \\\n",
    "    -lr $lr_input.value -comp_info $comp_info_input.value -epoch $epoch_input.value \\\n",
    "    -unisize $unisize_input.value -batch_size $batch_size_input.value -rgb_root $dataset_path\n",
    "\n",
    "def delete(dir, pattern):\n",
    "    for f in os.listdir(dir):\n",
    "        if re.search(pattern, f):\n",
    "            os.remove(os.path.join(dir, f))\n",
    "\n",
    "display(title, label, datasetDropDown, mode_input, model_input, train_input, num_clips_input, skip_input, lr_input, comp_info_input, epoch_input, unisize_input, alpha_l_input,beta_l_input, batch_size_input, button)\n",
    "button.on_click(selectWidgetSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3552607c",
   "metadata": {},
   "source": [
    "## Evaluation \n",
    "In this section we will be evulating the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e27fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model\n",
    "modelList = [] \n",
    "\n",
    "# Select from the list of model in the pipeline/models folder\n",
    "for x in os.listdir(\"./save_logit\"): \n",
    "    if os.path.isdir(os.path.join(\"./save_logit\", x)) == False:\n",
    "        if (x.endswith(\".pkl\")):\n",
    "            modelList.append(x)\n",
    "\n",
    "modelDropdown = widgets.Dropdown(\n",
    "    options=modelList,\n",
    "    value=modelList[0],\n",
    "    description='Model:'\n",
    "    )\n",
    "\n",
    "# Widgets\n",
    "confirmButton = widgets.Button(\n",
    "    description='Confirm',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    "    icon='check'\n",
    "    )\n",
    " \n",
    "# Function on what happen when confirm is been click.\n",
    "def selectWidgetSet(b):\n",
    "    print(\"Selected: \" , modelDropdown.value)\n",
    "    model_path = os.path.join(\"./save_logit/\" , modelDropdown.value)\n",
    "    print(\"Model Path: \", model_path)\n",
    "    %run -i Evaluation.py -pkl_path $model_path\n",
    "\n",
    "confirmButton.on_click(selectWidgetSet)\n",
    "modelBox = widgets.VBox([widgets.HBox([modelDropdown, confirmButton])])\n",
    "modelBox"
   ]
  }
 ],
 "metadata": {
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f4df3a7276bd7fa142fdb16837d91f8e4c668c8ba404b435530017c3f782edca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

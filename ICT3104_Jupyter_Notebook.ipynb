{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c205650",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3aebdb",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "Description: this cell is used for importing the relevant libraries for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae75fb79",
   "metadata": {
    "hide_input": false,
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload File using ipyfilechooser library\n",
    "from ipyfilechooser import FileChooser\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from tqdm.notebook import tqdm, trange\n",
    "# Video Player\n",
    "from IPython.display import Video, display, clear_output\n",
    "import time \n",
    "# Get the root directory of the project\n",
    "from pyprojroot import here\n",
    "# Copy File\n",
    "import shutil\n",
    "# Widget Packages\n",
    "import ipywidgets as widgets\n",
    "# In case widget extension not working\n",
    "# jupyter nbextension enable --py widge|tsnbextension\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv()\n",
    "os.environ['WANDB_API_KEY'] = os.getenv('WANDB_API_KEY')\n",
    "global_path = here(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7cb0ea",
   "metadata": {},
   "source": [
    "Description: This cell is used for opening the file selector for user to select the video as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7459f1aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def videoselectorinput():\n",
    "    starting_directory = '../data/video'\n",
    "    chooser = FileChooser(starting_directory)\n",
    "    display(chooser)\n",
    "    return chooser\n",
    "def videoselectoroutputTSU():\n",
    "    starting_directory = './video/output'\n",
    "    chooser = FileChooser(starting_directory)\n",
    "    display(chooser)\n",
    "    return chooser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd71b6b",
   "metadata": {},
   "source": [
    "## Pipeline Selection\n",
    "R6(Story): As a user, I want to create appropriate UI elements to allow for switching pipelines so that I can test the different models.\n",
    "\n",
    "### Adding new pipeline\n",
    "To add a new pipeline, add in the name of the pipeline in the options list in the dropdown widgets.\n",
    "\n",
    "T05-157 (Story): As a user, I want to run the pipeline based on the dropdown selected. So that, I can integrate the pipeline dropdown into the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7527021f",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "Description: This cell is used for setting and opening up the dropdown for the user to select the pipeline. After selecting and clicking \"Confirm\" the code will then run the selected pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205ee99d",
   "metadata": {
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(os.getcwd())\n",
    "pipelineList = ['TSU', 'STEP']\n",
    "pipelineDropdown = widgets.Dropdown(\n",
    "\n",
    "    options=pipelineList,\n",
    "    value=pipelineList[0],\n",
    "    description='Pipeline:')\n",
    "# Function on what happen when confirm is been click.\n",
    "def selectPipelineSet(b):\n",
    "    if pipelineDropdown.value == \"STEP\":\n",
    "        directory = os.getcwd()\n",
    "        # Use Regex to match STEP\n",
    "        if re.search(r\"TSU\", directory):\n",
    "            %cd ..\n",
    "            path =here(\"./NVIDIA-STEP-MODEL/STEP\")\n",
    "            %cd $path\n",
    "            \n",
    "        elif re.search(r\"STEP\", directory):\n",
    "            pass\n",
    "        else:\n",
    "            path =here(\"./NVIDIA-STEP-MODEL/STEP\")\n",
    "            %cd $path\n",
    "        print(\"Running \" , pipelineDropdown.value, \" pipeline\")\n",
    "        print(\"Loaded dependencies\")\n",
    "        #print(\"Please skip to Section 6 STEP Pipeline\")\n",
    "    elif pipelineDropdown.value == \"TSU\":\n",
    "        directory = os.getcwd()\n",
    "        # Use Regex to match STEP\n",
    "        if re.search(r\"TSU\", directory):\n",
    "            pass\n",
    "        elif re.search(r\"STEP\", directory):\n",
    "            %cd ../..\n",
    "            path =here(\"./pipeline\")\n",
    "            %cd $path\n",
    "        else:\n",
    "            path =here(\"./pipeline\")\n",
    "            %cd $path\n",
    "        print(\"Running \" , pipelineDropdown.value, \" pipeline\")\n",
    "        print(\"Loaded dependencies\")\n",
    "        path =here(\"./pipeline\")\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "pipelineConfirm = widgets.Button(\n",
    "    description='Confirm',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "pipelineConfirm.on_click(selectPipelineSet)\n",
    "pipelineBox = widgets.VBox([widgets.HBox([pipelineDropdown, pipelineConfirm])])\n",
    "pipelineBox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b17f71",
   "metadata": {},
   "source": [
    "<a href=\"#STEP\">Click here to go to STEP section</a>\n",
    "<br>\n",
    "<a href=\"#TSU\">Click here to go to TSU section</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68b99c7",
   "metadata": {},
   "source": [
    "# Data Exploration Section\n",
    "R2 (Epic): As a user, I want a \"Data Exploration\" section in the notebook so that I can load and display video data from the TSU project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed518c9e",
   "metadata": {},
   "source": [
    "## Video Upload / Choose using ipyfilechooser\n",
    "R2 (Story): As a user, I want to upload/choose files from the data folder through an appropriate UI component (E.g. Browse files) in a notebook code cell so that I can pick and choose the video data I would like to process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46254d0",
   "metadata": {},
   "source": [
    "Description: This cell is used for setting up the file choosers to select input video and to save output video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9251f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "video = videoselectorinput()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe9761f",
   "metadata": {},
   "source": [
    "## Upload selected video to the data folder (If needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e339780e",
   "metadata": {},
   "source": [
    "Description: This cell is used for uploading the video to data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cdc476",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Upload Function\n",
    "# from pyprojroot import here\n",
    "# import shutil\n",
    "def upload(video):\n",
    "    print(video.selected)\n",
    "    source = video.selected\n",
    "    # Source path\n",
    "    # Destination path\n",
    "    destination = (here(\"./data/video\"))\n",
    "\n",
    "    # Copy file from the selected path\n",
    "    try:\n",
    "        shutil.copy(source, destination)\n",
    "        print(\"File copied successfully.\")\n",
    "\n",
    "    # If source and destination are same\n",
    "    except shutil.SameFileError:\n",
    "        print(\"Source and destination represents the same file.\")\n",
    "\n",
    "    # If destination is a directory.\n",
    "    except IsADirectoryError:\n",
    "        print(\"Destination is a directory.\")\n",
    "\n",
    "    # If there is any permission issue\n",
    "    except PermissionError:\n",
    "        print(\"Permission denied.\")\n",
    "\n",
    "    # For other errors\n",
    "    except:\n",
    "        print(\"Error occurred while copying file.\")\n",
    "\n",
    "upload(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd190ad",
   "metadata": {},
   "source": [
    "## Video Playback\n",
    "R2 (Story): As a user, I want to see video playback of the chosen video file in an output cell so that I can check if it is the right video data I would like to process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b00bf6",
   "metadata": {},
   "source": [
    "Description: This cell is used for opening the file selector for user to select video input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ce6882",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select Video\n",
    "video = videoselectorinput()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73380e7d",
   "metadata": {},
   "source": [
    "Description: This cell is used for running the input video. This is for the user to check if it is the same video they selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09127163",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_path = video.selected\n",
    "Video(full_path, embed=True, width=540, html_attributes=\"controls muted autoplay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51191d83",
   "metadata": {},
   "source": [
    "<a id='TSU'></a>\n",
    "# Inference Section\n",
    "\n",
    "R3 (Epic): As a user, I want to have an \"Inference\" section in the notebook so that I can perform inference using a pretrained HOI ML model based on the TSU project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90b56cd",
   "metadata": {},
   "source": [
    "## Load a model\n",
    "R3 (Story): As a user, I want to load a pre-trained or trained model using an appropriate UI component so that I can easily load the model for the inference section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50fb0b0",
   "metadata": {},
   "source": [
    "Description: This cell is used for setting up the dropdown list to allow the user to select the model to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263f4135",
   "metadata": {
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select model\n",
    "modelList = [] \n",
    "\n",
    "# Select from the list of model in the pipeline/models folder\n",
    "for x in os.listdir(\"./models\"): \n",
    "    if os.path.isdir(os.path.join(\"./models\", x)) == False:\n",
    "        if (x.endswith(\".pkl\")) == False:\n",
    "            modelList.append(x)\n",
    "\n",
    "# Widgets\n",
    "confirmButton = widgets.Button(\n",
    "    description='Confirm',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    "    icon='check'\n",
    ")\n",
    "modelDropdown = widgets.Dropdown(\n",
    "    options=modelList,\n",
    "    value=modelList[0],\n",
    "    description='Model:')\n",
    "# Function on what happen when confirm is been click.\n",
    "def selectWidgetSet(b):\n",
    "    print(\"Selected: \" , modelDropdown.value)\n",
    "\n",
    "confirmButton.on_click(selectWidgetSet)\n",
    "modelBox = widgets.VBox([widgets.HBox([modelDropdown, confirmButton])])\n",
    "modelBox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cad9e82",
   "metadata": {},
   "source": [
    "## Choose Input video to load into TSU Project\n",
    "R3 (Story): As a user, I want to choose an  input video files and other related input files, using an appropriate UI component, from the TSU project so that the system is able to pass the right files to the model.\n",
    "\n",
    "Take Note: You only can select video that is in **testing subset** on the smarthome_CS_51.json file to run the inference video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9054f185",
   "metadata": {},
   "source": [
    "Description: This cell is used for selecting the video input to be passed to the TSU model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a7e1db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select Video\n",
    "video = videoselectorinput()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8854da",
   "metadata": {},
   "source": [
    "## Run the model \n",
    "R3 (Story): As a user, I want to see inference results in the form of an output video with captions that indicate the current detected activity in each video frame so that I am able to see the inference results clearly on the screen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0c9682",
   "metadata": {},
   "source": [
    "Description: This cell is used for running the inference.py script based on the video and model selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b5f2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = modelDropdown.value\n",
    "loadmodel = './models/' + model\n",
    "videoPath = video.selected\n",
    "videoFile = video.selected_filename\n",
    "print(videoFile)\n",
    "print(videoPath)\n",
    "%run -it inference.py  -input_video_file $videoFile -model $model  -load_model $loadmodel -video_path $videoPath\n",
    "%wandb ict3104-team-5/inference-visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d326ce9c",
   "metadata": {},
   "source": [
    "## Output Video to view the inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569c1dc6",
   "metadata": {},
   "source": [
    "Description: This cell is used for print the filepath of the output video. Play the output video below the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afb7c95",
   "metadata": {
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "videoFileName = video.selected_filename[:-4]\n",
    "full_path = (os.path.join(here(\"./pipeline/video/output/\"),f\"{videoFileName}_caption.mp4\"))\n",
    "print(full_path)\n",
    "Video(full_path, embed=True, width=540, html_attributes=\"controls muted autoplay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6935d29c",
   "metadata": {},
   "source": [
    "# Feature Extraction Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87c12f9",
   "metadata": {},
   "source": [
    "## Running main feature-extraction function\n",
    "Description: This cell is used for running the main feature-extraction function to extract features from videos listed in video_paths.txt to create RGB .npy files for training later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99b994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(global_path , 'i3d-feature-extraction') \n",
    "%cd $path\n",
    "%run -it main.py feature_type=i3d device=\"cuda:0\" on_extraction=save_numpy streams=rgb output_path=../data/dataset/v_iashin_i3d/ file_with_video_paths=./sample/sample_video_paths.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c593e13",
   "metadata": {},
   "source": [
    "## Running validate_train_test.py\n",
    "Description: This cell is used for running validate_train_test.py to remove video IDs from smarthome_cs_51.json file and create an updated version called smarthome_cs_51_v2.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba5d4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python validate_train_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67463a88",
   "metadata": {},
   "source": [
    "# Training Section\n",
    "R4 (Epic): As a user, I want to create a \"Training\" section in the netbook so that I can train a HOI ML model based on the TSU project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e20405",
   "metadata": {},
   "source": [
    "## Run_PDAN.sh for training\n",
    "R4 (Story): As a user, I want to able to change the value for the argument in run_PDAN shell script with a UI so that I does not need to keep changing the value directly in the shell script.\n",
    "\n",
    "R4 (Story): As a user, I can choose a dataset subfolder, using appropriate UI elements, from the data folder to use for the training so that I can select the data for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3ae2eb",
   "metadata": {},
   "source": [
    "Description: This cell is used for form to fill up the parameters needed to run the run_PDAN.sh for training. Running the run_PDAN.sh script based on the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d10fe28",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "title = widgets.HTML(\"<h1 style=\\\"text-align: center; \\\"><b>Run_PDAN</b></h1>\")\n",
    "label = widgets.HTML(\n",
    "    value=\"<b>Label</b>\\\n",
    "    <br><b>Dataset Input:</b> Name of the Dataset to train\\\n",
    "    <br><b>Mode:</b> Inference Metric Methods: rgb or flow\\\n",
    "    <br><b>Spilt Setting:</b> Either CS or CV\\\n",
    "    <br><b>Model:</b> Select Model To Train\\\n",
    "    <br><b>Train:</b> Train or Test\\\n",
    "    <br><b>Num Channel:</b> Number of channel for image processing\\\n",
    "    <br><b>Learning Rate:</b> Learning rate indicates the step size that gradient descent takes towards local optima\\\n",
    "    <br><b>Kernel Size:</b> Size of a convolutional filter\\\n",
    "    <br><b>AP Type:</b> Metric in measuring the accuracy of object detectors: map or wap\\\n",
    "    <br><b>Epoch:</b> Number of iterations for the training process in Machine Learning\\\n",
    "    <br><b>Batch_Size:</b> Number of samples that is used in one epoch.\\\n",
    "    <br><b>Compute Info:</b> Information of the computation\"\n",
    ")\n",
    "style = {'description_width': '90px'}\n",
    "\n",
    "# Using Dropdown method\n",
    "dataset_list = []\n",
    "directoryDataset = os.path.join(global_path , 'data' , 'dataset') # Directory of the dataset (NPY files)\n",
    "# Store the folder in the  dataset into dataset list\n",
    "for x in os.listdir(directoryDataset):\n",
    "    if os.path.isdir(os.path.join(directoryDataset, x)):\n",
    "        # print(os.path.join(directoryDataset, x))\n",
    "        dataset_list.append(x)\n",
    "\n",
    "datasetDropDown = widgets.Dropdown(\n",
    "    options=dataset_list,\n",
    "    value=dataset_list[0],\n",
    "    style=style,\n",
    "    description='Dataset:')\n",
    "\n",
    "\n",
    "mode_input = widgets.Text(\n",
    "    value='rgb',\n",
    "    placeholder='Enter Mode',\n",
    "    description='Mode:',\n",
    "    disabled=False,\n",
    "    style=style,\n",
    "    tooltip='Inference Metric Methods: rgb or flow'\n",
    "    \n",
    ")\n",
    "\n",
    "split_input = widgets.Text(\n",
    "    value='CS',\n",
    "    placeholder='Enter Split Setting',\n",
    "    description='Split Setting:',\n",
    "    disabled=False,\n",
    "    style=style,\n",
    "    tooltip='Either CS or CV'\n",
    ")\n",
    "\n",
    "# Using Dropdown method\n",
    "model_list = []\n",
    "model_list.append(\"PDAN_TSU_RGB\")\n",
    "        \n",
    "model_input = widgets.Dropdown(\n",
    "    options=model_list,\n",
    "    value=model_list[0],\n",
    "    description='Model:',\n",
    "    style=style,\n",
    "    tooltip='Select Model to train.'\n",
    ")\n",
    "\n",
    "\n",
    "train_input = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description = 'Train',\n",
    "    disabled=False,\n",
    "    indent=True,\n",
    "    style=style,\n",
    "    tooltip='Train or Test'\n",
    ")\n",
    "\n",
    "num_channel_input =  widgets.BoundedIntText(\n",
    "    value=512,\n",
    "    min=1,\n",
    "    max=1000,\n",
    "    step=1,\n",
    "    description='Num Channel:',\n",
    "    disabled=False,\n",
    "    style=style,\n",
    "    tooltip='Number of channel for image processing'\n",
    ")\n",
    "\n",
    "lr_input = widgets.FloatText(\n",
    "    value=0.0002,\n",
    "    description='Learning Rate:',\n",
    "    disabled=False,\n",
    "    style=style,\n",
    "    tooltip='Learning rate indicates the step size that gradient descent takes towards local optima'\n",
    ")\n",
    "\n",
    "kernel_size_input =  widgets.BoundedIntText(\n",
    "    value=3,\n",
    "    min=1,\n",
    "    max=5,\n",
    "    step=1,\n",
    "    description='Kernel Size:',\n",
    "    disabled=False,\n",
    "    style=style,\n",
    "    tooltip='Size of a convolutional filter'\n",
    ")\n",
    "\n",
    "aptype_input = widgets.Text(\n",
    "    value='map',\n",
    "    placeholder='Enter APType',\n",
    "    description='APType:',\n",
    "    disabled=False,\n",
    "    style=style,\n",
    "    tooltip='Metric in measuring the accuracy of object detectors: map or wap'\n",
    ")\n",
    "\n",
    "epoch_input = widgets.BoundedIntText(\n",
    "    value=140,\n",
    "    min=1,\n",
    "    max=1000,\n",
    "    step=1,\n",
    "    description='Epoch:',\n",
    "    disabled=False,\n",
    "    style=style,\n",
    "    tooltip='Number of iterations for the training process in Machine Learning'\n",
    ")\n",
    "\n",
    "batch_size_input = widgets.Dropdown(\n",
    "    options=['1', '2', '4', '8', '16', '32', '64', '128', '256', '512', '1024'],\n",
    "    value='1',\n",
    "    description='Batch_Size:',\n",
    "    disabled=False,\n",
    "    tooltip='Number of samples that is used in one epoch.',\n",
    "    style=style\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "comp_info_input = widgets.Text(\n",
    "    value='TSU_CS_RGB_PDAN',\n",
    "    placeholder='Enter Compute Info',\n",
    "    description='Compute Info:',\n",
    "    disabled=False,\n",
    "    style=style,\n",
    "    tooltip='Information of the computation.'\n",
    ")\n",
    "\n",
    "button = widgets.Button(\n",
    "    description='Save',\n",
    "    disabled=False,\n",
    "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Save Pref',\n",
    "    icon='check', # (FontAwesome names without the `fa-` prefix)\n",
    "    style=style\n",
    ")\n",
    "\n",
    "jsonDirectory = os.path.join(directoryDataset , 'JSON')\n",
    "jsonFiles = []\n",
    "for x in os.listdir(jsonDirectory):\n",
    "    if os.path.isdir(os.path.join(jsonDirectory, x)) == False:\n",
    "        jsonFiles.append(x)\n",
    "\n",
    "json_file_input = widgets.Dropdown(\n",
    "    options=jsonFiles,\n",
    "    value=jsonFiles[0],\n",
    "    style=style,\n",
    "    description='Json File:')\n",
    "    \n",
    "\n",
    "def selectWidgetSet(b):\n",
    "    print(\"Selected Dataset: \" , datasetDropDown.value)\n",
    "    print(\"Selected Mode: \" , mode_input.value)\n",
    "    print(\"Selected Split Setting: \" , split_input.value)\n",
    "    print(\"Selected Model: \" , model_input.value)\n",
    "    print(\"Selected Train: \" , train_input.value)\n",
    "    print(\"Selected Num Channel: \" , num_channel_input.value)\n",
    "    print(\"Selected Learning Rate: \" , lr_input.value)\n",
    "    print(\"Selected Kernel Size: \" , kernel_size_input.value)\n",
    "    print(\"Selected APType: \", aptype_input.value)\n",
    "    print(\"Selected Epoch: \", epoch_input.value)\n",
    "    print(\"Selected Batch: \" , batch_size_input.value)\n",
    "    print(\"Selected Compute Info: \" , comp_info_input.value)\n",
    "    dataset_path = os.path.join(\"../data/dataset/\" , datasetDropDown.value)\n",
    "    print(\"Dataset Path: \", dataset_path)\n",
    "    json_path = os.path.join(\"../data/dataset/JSON/\" , json_file_input.value)\n",
    "    print(\"Json Path: \", json_path)\n",
    "    %run -i train.py -dataset $datasetDropDown.value -mode $mode_input.value -split_setting $split_input.value -model $model_input.value -train $train_input.value -num_channel $num_channel_input.value -lr $lr_input.value -kernelsize $kernel_size_input.value -APtype $aptype_input.value -epoch $epoch_input.value -batch_size $batch_size_input.value -comp_info $comp_info_input.value -rgb_root $dataset_path -json_file $json_path\n",
    "    %wandb ict3104-team-5/training-visualisation\n",
    "\n",
    "display(title, label, datasetDropDown, mode_input,split_input, model_input, train_input, num_channel_input, lr_input, kernel_size_input, aptype_input, epoch_input, batch_size_input, comp_info_input, json_file_input ,button)\n",
    "button.on_click(selectWidgetSet) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba99aa7",
   "metadata": {},
   "source": [
    "# Testing Section\n",
    "R5 (Epic): As a user, I want to have a \"Testing\" section in the notebook so that I can evaluate a trained model based on the TSU project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5bfc3b",
   "metadata": {},
   "source": [
    "R5 (Story): As a user, I want to load a pretrained model using an appropriate UI component so that I can easily choose the type of pretrained model I would like to process the data with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e87d280",
   "metadata": {},
   "source": [
    "Description: This cell is used for dropdown for the user to select the model to be tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6ef6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model\n",
    "modelList = [] \n",
    "\n",
    "# Select from the list of model in the pipeline/models folder\n",
    "for x in os.listdir(\"./models\"): \n",
    "    if os.path.isdir(os.path.join(\"./models\", x)) == False:\n",
    "        if (x.endswith(\".pkl\")):\n",
    "            modelList.append(x)\n",
    "\n",
    "# Widgets\n",
    "confirmButton = widgets.Button(\n",
    "    description='Confirm',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    "    icon='check'\n",
    ")\n",
    "modelDropdown = widgets.Dropdown(\n",
    "    options=modelList,\n",
    "    value=modelList[0],\n",
    "    description='Model:')\n",
    "# Function on what happen when confirm is been click.\n",
    "def selectWidgetSet(b):\n",
    "    print(\"Selected: \" , modelDropdown.value)\n",
    "\n",
    "confirmButton.on_click(selectWidgetSet)\n",
    "modelBox = widgets.VBox([widgets.HBox([modelDropdown, confirmButton])])\n",
    "modelBox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b674a8",
   "metadata": {},
   "source": [
    "R5 (Epic): As a user, I want to have a \"Testing\" section in the notebook so that I can evaluate a trained model based on the TSU project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31541446",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i test.py -split CS -pkl_path \"models/\"$modelDropdown.value -batch_size batch_size_input.value -learning_rate lr_input.value -epochs epoch_input.value\n",
    "%wandb ict3104-team-5/testing-visualisation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11c52f5",
   "metadata": {},
   "source": [
    "# Inference Section for Training model\n",
    "\n",
    "R3 (Epic): As a user, I want to have an \"Inference\" section in the notebook so that I can perform inference using a pretrained HOI ML model based on the TSU project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1aebcb",
   "metadata": {
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select model\n",
    "modelList = [] \n",
    "\n",
    "# Select from the list of model in the pipeline/models folder\n",
    "for x in os.listdir(\"./models\"): \n",
    "    if os.path.isdir(os.path.join(\"./models\", x)) == False:\n",
    "        if (x.endswith(\".pkl\")) == False:\n",
    "            modelList.append(x)\n",
    "\n",
    "# Widgets\n",
    "confirmButton = widgets.Button(\n",
    "    description='Confirm',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    "    icon='check'\n",
    ")\n",
    "modelDropdown = widgets.Dropdown(\n",
    "    options=modelList,\n",
    "    value=modelList[0],\n",
    "    description='Model:')\n",
    "# Function on what happen when confirm is been click.\n",
    "def selectWidgetSet(b):\n",
    "    print(\"Selected: \" , modelDropdown.value)\n",
    "    model = modelDropdown.value\n",
    "    loadmodel = './models/' + model\n",
    "    videoPath = video.selected\n",
    "    videoFile = video.selected_filename\n",
    "    print(videoFile)\n",
    "    print(videoPath)\n",
    "    %run -it inference.py  -input_video_file $videoFile -model $model  -load_model $loadmodel -video_path $videoPath\n",
    "    %wandb ict3104-team-5/inference-visualisation\n",
    "\n",
    "confirmButton.on_click(selectWidgetSet)\n",
    "modelBox = widgets.VBox([widgets.HBox([modelDropdown, confirmButton])])\n",
    "modelBox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaa05ef",
   "metadata": {},
   "source": [
    "# Output Video for Inference Section for Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908d264b",
   "metadata": {
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "videoFileName = video.selected_filename[:-4]\n",
    "full_path = (os.path.join(here(\"./pipeline/video/output/\"),f\"{videoFileName}_caption.mp4\"))\n",
    "print(full_path)\n",
    "Video(full_path, embed=True, width=540, html_attributes=\"controls muted autoplay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5143aac1",
   "metadata": {},
   "source": [
    "# NVIDIA STEP Section\n",
    "R6 (Epic): As a user, I want to able to configure the notebook using appropriate UI elements coupled with the right .py modules so that R2-5 can be performed based on another pipeline, e.g., the NVIDIA STEP pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58612b38",
   "metadata": {},
   "source": [
    "<a id='STEP'></a>\n",
    "## Nvidia Step Pipeline\n",
    "R6(Story): As a user, I want to ensure selected pipeline's dependencies are changed to ensure the right dependencies are given to the appropriate models so that the selected model will be run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566bc6f4",
   "metadata": {},
   "source": [
    "Description: This cell is used for setup the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f12f59e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Else run this\n",
    "%run -i setup.py build develop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd78fe48",
   "metadata": {},
   "source": [
    "T05-128: As a User, I want pre-process the input video into frames for Nvidia STEP model, So that I can run the model successfully.\n",
    "\n",
    "T05-127 As a User, I want the output of Nvidia STEP model to be in a video format, So that I can easily view the results of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75a7bbc",
   "metadata": {},
   "source": [
    "T05-119: As a user, I want to have multiple dropdowns to select the input (1) dataset (2) video so that I can select the dataset videos easily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0728041",
   "metadata": {},
   "source": [
    "Description: This cell is used for setting and  opening up the dropdowns to allow user to select the dataset and the video input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18281ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select video\n",
    "video_list = [] \n",
    "dataset_list = [] \n",
    "directoryVideo = \"./Input/\" # Directory of the video (.mp4)\n",
    "directoryDataset = \"./datasets/demo/frames/\" # Directory of the dataset (NPY files)\n",
    "\n",
    "# Store the folder in the  dataset into dataset list\n",
    "for x in os.listdir(directoryDataset):\n",
    "    if os.path.isdir(os.path.join(directoryDataset, x)):\n",
    "        # print(os.path.join(directoryDataset, x))\n",
    "        dataset_list.append(x)\n",
    "\n",
    "# Store the video names into video list\n",
    "for x in os.listdir(directoryVideo):\n",
    "    if x.endswith(\".mp4\"):\n",
    "        video_list.append(x)\n",
    "\n",
    "datasetDropDown = widgets.Dropdown(\n",
    "    options=dataset_list,\n",
    "    description='Dataset:')\n",
    "\n",
    "videoDropDown = widgets.Dropdown(\n",
    "    options=video_list,\n",
    "    description='Video: ')\n",
    "\n",
    "# Widgets\n",
    "confirmButton = widgets.Button(\n",
    "    description='Confirm',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    "    icon='check'\n",
    ")\n",
    "# Function on what happen when confirm is been click. To be intergrated with some other code\n",
    "def selectTrimSet(b):\n",
    "    print(\"Selected Dataset: \" , datasetDropDown.value)\n",
    "    print(\"Selected Video: \" , videoDropDown.value)\n",
    "\n",
    "confirmButton.on_click(selectTrimSet)\n",
    "display(datasetDropDown, videoDropDown, confirmButton)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f1ba03",
   "metadata": {},
   "source": [
    "Description: This cell is used to caption every frame of the video. Afterwards, it will run the STEP Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c0f48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "cap= cv2.VideoCapture('Input/' + videoDropDown.value)\n",
    "i=0\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret == False:\n",
    "        break\n",
    "    cv2.imwrite('datasets/demo/frames/' + datasetDropDown.value + '/frame'+str(i).zfill(4)+'.jpg',frame)\n",
    "    i+=1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "## Traubung code will be here\n",
    "%run -i demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23d9f6d",
   "metadata": {},
   "source": [
    "Description: This cell is used for reading every frames in the results folder.It will write the captions of the frames onto the output video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9872d574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "img_array = []\n",
    "for filename in glob.glob('datasets/demo/results/' + datasetDropDown.value + '/*.jpg'):\n",
    "    img = cv2.imread(filename) \n",
    "    height, width, layers = img.shape\n",
    "    size = (width,height)\n",
    "    img_array.append(img)\n",
    "\n",
    "\n",
    "out = cv2.VideoWriter('Output/demo-result.mp4',cv2.VideoWriter_fourcc(*'MP4V'), 15, size)\n",
    " \n",
    "for i in range(len(img_array)):\n",
    "    out.write(img_array[i])\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37ee6ec",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbfd998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014eac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./scripts/generate_label.py \"datasets/ava/label/ava_train_v2.1.csv\" \n",
    "%run ./scripts/generate_label.py \"datasets/ava/label/ava_val_v2.1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de72af6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run ./scripts/extract_clips.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a30ff7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_root=\"datasets/ava/\"\n",
    "save_root=\"datasets/ava/cache/\"\n",
    "pretrain_path=\"pretrained/ava_step.pth\"\n",
    "\n",
    "name=\"STEP\"\n",
    "base_net=\"i3d\"\n",
    "det_net=\"two_branch\"\n",
    "resume_path=\"Auto\"\n",
    "\n",
    "T=3\n",
    "max_iter=3    # index starts from 1\n",
    "iterative_mode=\"temporal\"\n",
    "anchor_mode=\"1\"\n",
    "temporal_mode=\"predict\"\n",
    "pool_mode=\"align\"\n",
    "pool_size=7\n",
    "\n",
    "# training schedule\n",
    "num_workers=2\n",
    "max_epochs=144\n",
    "\n",
    "batch_size=1\n",
    "optimizer=\"adam\"\n",
    "base_lr=7.5e-5\n",
    "det_lr0=1.5e-4\n",
    "det_lr=7.5e-4\n",
    "save_step=11465\n",
    "print_step=500\n",
    "scheduler=\"cosine\"\n",
    "milestones=\"-1\"\n",
    "warmup_iters=1000\n",
    "\n",
    "# losses\n",
    "dropout=0.3\n",
    "fc_dim=256\n",
    "lambda_reg=5\n",
    "lambda_neighbor=1\n",
    "cls_thresh=\"0.2,0.35,0.5\"\n",
    "reg_thresh=\"0.2,0.35,0.5\"\n",
    "max_pos_num=5\n",
    "neg_ratio=2\n",
    "NUM_SAMPLE=-1\n",
    "topk=300\n",
    "evaluate_topk=300\n",
    "\n",
    "# data augmentation / normalization\n",
    "scale_norm=2    # for i3d\n",
    "do_flip=\"True\"\n",
    "do_crop=\"True\"\n",
    "do_photometric=\"True\"\n",
    "do_erase=\"True\"\n",
    "freeze_affine=\"True\"\n",
    "freeze_stats=\"True\"\n",
    "\n",
    "\n",
    "%run train.py --data_root $data_root --save_root $save_root \\\n",
    "    --name $name --pretrain_path $pretrain_path --resume_path $resume_path \\\n",
    "    --base_net $base_net --det_net $det_net --max_iter $max_iter --T $T \\\n",
    "    --iterative_mode $iterative_mode --anchor_mode $anchor_mode --anchor_mode $anchor_mode --temporal_mode $temporal_mode \\\n",
    "    --pool_mode $pool_mode --pool_size $pool_size --save_step $save_step --topk $topk --evaluate_topk $evaluate_topk \\\n",
    "    --num_workers $num_workers --max_epochs $max_epochs --batch_size $batch_size --print_step $print_step \\\n",
    "    --optimizer $optimizer --base_lr $base_lr --det_lr $det_lr --det_lr0 $det_lr0 --milestones $milestones \\\n",
    "    --scale_norm $scale_norm --do_flip $do_flip --do_crop $do_crop --do_photometric $do_photometric --do_erase $do_erase \\\n",
    "    --fc_dim $fc_dim --dropout $dropout --NUM_SAMPLE $NUM_SAMPLE --scheduler $scheduler --warmup_iters $warmup_iters \\\n",
    "    --cls_thresh $cls_thresh --reg_thresh $reg_thresh --max_pos_num $max_pos_num --neg_ratio $neg_ratio \\\n",
    "    --freeze_affine $freeze_affine --freeze_stats $freeze_stats --lambda_reg $lambda_reg --lambda_neighbor $lambda_neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103c75ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_root=\"datasets/ava/\" #pickle name\n",
    "save_root=\"datasets/ava/cache/\" #\n",
    "kinetics_pretrain=\"pretrained/i3d_kinetics.pth\"\n",
    "\n",
    "name=\"Cls\"\n",
    "base_net=\"i3d\"\n",
    "det_net=\"two_branch\"\n",
    "resume_path=\"Auto\"\n",
    "\n",
    "T=9\n",
    "max_iter=1    # index starts from 1\n",
    "iterative_mode=\"spatial\"\n",
    "pool_mode=\"align\"\n",
    "pool_size=7\n",
    "\n",
    "# training schedule\n",
    "num_workers=1\n",
    "max_epochs=14\n",
    "batch_size=1\n",
    "optimizer=\"adam\"\n",
    "base_lr=5e-5\n",
    "det_lr0=1e-4\n",
    "det_lr=5e-4\n",
    "save_step=22930\n",
    "print_step=2000\n",
    "scheduler=\"cosine\"\n",
    "milestones=\"-1\"\n",
    "warmup_iters=1000\n",
    "\n",
    "# losses\n",
    "dropout=0.3\n",
    "fc_dim=256\n",
    "\n",
    "# data augmentation / normalization\n",
    "scale_norm=2    # for i3d\n",
    "do_flip=\"True\"\n",
    "do_crop=\"True\"\n",
    "do_photometric=\"True\"\n",
    "do_erase=\"True\"\n",
    "freeze_affine=\"True\"\n",
    "freeze_stats=\"True\"\n",
    "\n",
    "\n",
    "%run train_cls.py --data_root $data_root --save_root $save_root \\\n",
    "    --name $name --resume_path $resume_path --kinetics_pretrain $kinetics_pretrain \\\n",
    "    --base_net $base_net --det_net $det_net --max_iter $max_iter --T $T \\\n",
    "    --iterative_mode $iterative_mode \\\n",
    "    --pool_mode $pool_mode --pool_size $pool_size --save_step $save_step \\\n",
    "    --num_workers $num_workers --max_epochs $max_epochs --batch_size $batch_size --print_step $print_step \\\n",
    "    --optimizer $optimizer --base_lr $base_lr --det_lr $det_lr --det_lr0 $det_lr0 --milestones $milestones \\\n",
    "    --scale_norm $scale_norm --do_flip $do_flip --do_crop $do_crop --do_photometric $do_photometric --do_erase $do_erase \\\n",
    "    --fc_dim $fc_dim --dropout $dropout  --scheduler $scheduler --warmup_iters $warmup_iters \\\n",
    "    --freeze_affine $freeze_affine --freeze_stats $freeze_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2ed053",
   "metadata": {},
   "source": [
    "## STEP Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ca107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run test.py"
   ]
  }
 ],
 "metadata": {
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "21443173980de8f8bd52f0a8d21aa24ff048531cbf1e79ef9f1f2c09c5456057"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

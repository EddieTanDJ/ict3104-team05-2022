{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c205650",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3aebdb",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "Description: this cell is used for importing the relevant libraries for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae75fb79",
   "metadata": {
    "hide_input": false,
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true,
    "tags": [
     "trial"
    ]
   },
   "outputs": [],
   "source": [
    "# Upload File using ipyfilechooser library\n",
    "from ipyfilechooser import FileChooser\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from tqdm.notebook import tqdm, trange\n",
    "# Video Player\n",
    "from IPython.display import Video, display, clear_output\n",
    "import time \n",
    "# Get the root directory of the project\n",
    "from pyprojroot import here\n",
    "# Copy File\n",
    "import shutil\n",
    "# Widget Packages\n",
    "import ipywidgets as widgets\n",
    "# In case widget extension not working\n",
    "# jupyter nbextension enable --py widge|tsnbextension\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import cv2\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "# load_dotenv()\n",
    "# os.environ['WANDB_API_KEY'] = os.getenv('WANDB_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7cb0ea",
   "metadata": {},
   "source": [
    "Description: This cell is used for opening the file selector for user to select the video as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7459f1aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def videoselectorinput():\n",
    "    starting_directory = './data/video'\n",
    "    chooser = FileChooser(starting_directory)\n",
    "    display(chooser)\n",
    "    return chooser\n",
    "def videoselectoroutput():\n",
    "    starting_directory = './pipeline/video/output'\n",
    "    chooser = FileChooser(starting_directory)\n",
    "    display(chooser)\n",
    "    return chooser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd71b6b",
   "metadata": {},
   "source": [
    "## Pipeline Selection\n",
    "R6(Story): As a user, I want to create appropriate UI elements to allow for switching pipelines so that I can test the different models.\n",
    "\n",
    "### Adding new pipeline\n",
    "To add a new pipeline, add in the name of the pipeline in the options list in the dropdown widgets.\n",
    "\n",
    "T05-157 (Story): As a user, I want to run the pipeline based on the dropdown selected. So that, I can integrate the pipeline dropdown into the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7527021f",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "Description: This cell is used for setting and opening up the dropdown for the user to select the pipeline. After selecting and clicking \"Confirm\" the code will then run the selected pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "205ee99d",
   "metadata": {
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fef290f96954ce3948e237a17a0759b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Pipeline:', options=('TSU', 'STEP'), value='TSU'), Button(â€¦"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sisters\\source\\repos\\ict3104-team05-2022\\NVIDIA-STEP-MODEL\\STEP\n",
      "Running  STEP  pipeline\n",
      "Loaded dependencies\n"
     ]
    }
   ],
   "source": [
    "#print(os.getcwd())\n",
    "pipelineList = ['TSU', 'STEP']\n",
    "pipelineDropdown = widgets.Dropdown(\n",
    "    options=pipelineList,\n",
    "    value=pipelineList[0],\n",
    "    description='Pipeline:')\n",
    "# Function on what happen when confirm is been click.\n",
    "def selectPipelineSet(b):\n",
    "    if pipelineDropdown.value == \"STEP\":\n",
    "        directory = os.getcwd()\n",
    "        # Use Regex to match STEP\n",
    "        if re.search(r\"TSU\", directory):\n",
    "            %cd ..\n",
    "            path =here(\"./NVIDIA-STEP-MODEL/STEP\")\n",
    "            %cd $path\n",
    "            \n",
    "        elif re.search(r\"STEP\", directory):\n",
    "            pass\n",
    "        else:\n",
    "            path =here(\"./NVIDIA-STEP-MODEL/STEP\")\n",
    "            %cd $path\n",
    "        print(\"Running \" , pipelineDropdown.value, \" pipeline\")\n",
    "        print(\"Loaded dependencies\")\n",
    "        #print(\"Please skip to Section 6 STEP Pipeline\")\n",
    "    elif pipelineDropdown.value == \"TSU\":\n",
    "        directory = os.getcwd()\n",
    "        # Use Regex to match STEP\n",
    "        if re.search(r\"TSU\", directory):\n",
    "            %cd pass\n",
    "        elif re.search(r\"STEP\", directory):\n",
    "            %cd ..\n",
    "            path =here(\"./pipeline\")\n",
    "            %cd $path\n",
    "        else:\n",
    "            path =here(\"./pipeline\")\n",
    "            %cd $path\n",
    "        print(\"Running \" , pipelineDropdown.value, \" pipeline\")\n",
    "        print(\"Loaded dependencies\")\n",
    "        path =here(\"./pipeline\")\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "pipelineConfirm = widgets.Button(\n",
    "    description='Confirm',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "pipelineConfirm.on_click(selectPipelineSet)\n",
    "pipelineBox = widgets.VBox([widgets.HBox([pipelineDropdown, pipelineConfirm])])\n",
    "pipelineBox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b17f71",
   "metadata": {},
   "source": [
    "<a href=\"#STEP\">Click here to go to STEP section</a>\n",
    "<br>\n",
    "<a href=\"#TSU\">Click here to go to TSU section</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68b99c7",
   "metadata": {},
   "source": [
    "# Data Exploration Section\n",
    "R2 (Epic): As a user, I want a \"Data Exploration\" section in the notebook so that I can load and display video data from the TSU project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed518c9e",
   "metadata": {},
   "source": [
    "## Video Upload / Choose using ipyfilechooser\n",
    "R2 (Story): As a user, I want to upload/choose files from the data folder through an appropriate UI component (E.g. Browse files) in a notebook code cell so that I can pick and choose the video data I would like to process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46254d0",
   "metadata": {},
   "source": [
    "Description: This cell is used for setting up the file choosers to select input video and to save output video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9251f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "video = videoselectorinput()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe9761f",
   "metadata": {},
   "source": [
    "## Upload selected video to the data folder (If needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e339780e",
   "metadata": {},
   "source": [
    "Description: This cell is used for uploading the video to data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cdc476",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Upload Function\n",
    "# from pyprojroot import here\n",
    "# import shutil\n",
    "def upload(video):\n",
    "    print(video.selected)\n",
    "    source = video.selected\n",
    "    # Source path\n",
    "    # Destination path\n",
    "    destination = (here(\"./data/video\"))\n",
    "\n",
    "    # Copy file from the selected path\n",
    "    try:\n",
    "        shutil.copy(source, destination)\n",
    "        print(\"File copied successfully.\")\n",
    "\n",
    "    # If source and destination are same\n",
    "    except shutil.SameFileError:\n",
    "        print(\"Source and destination represents the same file.\")\n",
    "\n",
    "    # If destination is a directory.\n",
    "    except IsADirectoryError:\n",
    "        print(\"Destination is a directory.\")\n",
    "\n",
    "    # If there is any permission issue\n",
    "    except PermissionError:\n",
    "        print(\"Permission denied.\")\n",
    "\n",
    "    # For other errors\n",
    "    except:\n",
    "        print(\"Error occurred while copying file.\")\n",
    "\n",
    "upload(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd190ad",
   "metadata": {},
   "source": [
    "## Video Playback\n",
    "R2 (Story): As a user, I want to see video playback of the chosen video file in an output cell so that I can check if it is the right video data I would like to process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b00bf6",
   "metadata": {},
   "source": [
    "Description: This cell is used for opening the file selector for user to select video input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ce6882",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select Video\n",
    "video = videoselectorinput()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0fea43",
   "metadata": {},
   "source": [
    "Description: This cell is used for printing the details of the selected video (the full path of the video, the name of the video and the directory the video is in). This is for the user to check if it is the same video they selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0d5e2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(video.selected)\n",
    "print(video.selected_filename)\n",
    "print(video.selected_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73380e7d",
   "metadata": {},
   "source": [
    "Description: This cell is used for running the input video. This is for the user to check if it is the same video they selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09127163",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_path = video.selected\n",
    "Video(full_path, embed=True, width=540, html_attributes=\"controls muted autoplay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51191d83",
   "metadata": {},
   "source": [
    "<a id='TSU'></a>\n",
    "# Inference Section\n",
    "\n",
    "R3 (Epic): As a user, I want to have an \"Inference\" section in the notebook so that I can perform inference using a pretrained HOI ML model based on the TSU project.\n",
    "<a href='#test'>Link to test</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90b56cd",
   "metadata": {},
   "source": [
    "## Load a pretrain model\n",
    "R3 (Story): As a user, I want to load a pre-trained model using an appropriate UI component so that I can easily load the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50fb0b0",
   "metadata": {},
   "source": [
    "Description: This cell is used for setting up the dropdown list to allow the user to select the model to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263f4135",
   "metadata": {
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select model\n",
    "path =here()\n",
    "os.chdir(path)\n",
    "modelList = [] \n",
    "\n",
    "# Select from the list of model in the pipeline/models folder\n",
    "for x in os.listdir(\"./pipeline/models\"): \n",
    "    modelList += [x]\n",
    "\n",
    "# Widgets\n",
    "confirmButton = widgets.Button(\n",
    "    description='Confirm',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    "    icon='check'\n",
    ")\n",
    "modelDropdown = widgets.Dropdown(\n",
    "    options=modelList,\n",
    "    value=modelList[0],\n",
    "    description='Model:')\n",
    "# Function on what happen when confirm is been click.\n",
    "def selectWidgetSet(b):\n",
    "    print(\"Selected: \" , modelDropdown.value)\n",
    "\n",
    "confirmButton.on_click(selectWidgetSet)\n",
    "modelBox = widgets.VBox([widgets.HBox([modelDropdown, confirmButton])])\n",
    "modelBox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cad9e82",
   "metadata": {},
   "source": [
    "## Choose Input video to load into TSU Project\n",
    "R3 (Story): As a user, I want to choose an  input video files and other related input files, using an appropriate UI component, from the TSU project so that the system is able to pass the right files to the model.\n",
    "\n",
    "Take Note: You only can select video that is in **testing subset** on the smarthome_CS_51.json file to run the inference video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9054f185",
   "metadata": {},
   "source": [
    "Description: This cell is used for selecting the video input to be passed to the TSU model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a7e1db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select Video\n",
    "video = videoselectorinput()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8854da",
   "metadata": {},
   "source": [
    "<a id='test'></a>\n",
    "## Run the model \n",
    "R3 (Story): As a user, I want to see inference results in the form of an output video with captions that indicate the current detected activity in each video frame so that I am able to see the inference results clearly on the screen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0c9682",
   "metadata": {},
   "source": [
    "Description: This cell is used for running the inference.py script based on the video and model selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b5f2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = here(\"./pipeline\")\n",
    "# %cd $path\n",
    "model = modelDropdown.value\n",
    "loadmodel = './models/' + model\n",
    "videoPath = video.selected\n",
    "videoFile = video.selected_filename\n",
    "print(videoFile)\n",
    "print(videoPath)\n",
    "%run -it inference.py  -input_video_file $videoFile -model $model  -load_model $loadmodel -video_path $videoPath\n",
    "%wandb ict3104-team-5/inference-visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d326ce9c",
   "metadata": {},
   "source": [
    "## Output Video to view the inference result "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569c1dc6",
   "metadata": {},
   "source": [
    "Description: This cell is used for print the filepath of the output video. Play the output video below the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afb7c95",
   "metadata": {
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "videoFileName = video.selected_filename[:-4]\n",
    "full_path = (os.path.join(here(\"./pipeline/video/output/\"),f\"{videoFileName}_caption.mp4\"))\n",
    "print(full_path)\n",
    "Video(full_path, embed=True, width=540, html_attributes=\"controls muted autoplay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6935d29c",
   "metadata": {},
   "source": [
    "# Feature Extraction Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87c12f9",
   "metadata": {},
   "source": [
    "## Running main feature-extraction function\n",
    "Description: This cell is used for running the main feature-extraction function to extract features from videos listed in video_paths.txt to create RGB .npy files for training later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99b994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = here(\"./i3d-feature-extraction\")\n",
    "\n",
    "%cd $path\n",
    "%run -i main.py feature_type=r21d device=\"cuda:0\" on_extraction=save_numpy streams=rgb output_path=./output/RGB_TEST file_with_video_paths=./sample/test_video_paths.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c593e13",
   "metadata": {},
   "source": [
    "## Running validate_train_test.py\n",
    "Description: This cell is used for running validate_train_test.py to remove video IDs from smarthome_cs_51.json file and create an updated version called smarthome_cs_51_v2.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba5d4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ./i3d-feature-extraction && python validate_train_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67463a88",
   "metadata": {},
   "source": [
    "# Training Section\n",
    "R4 (Epic): As a user, I want to create a \"Training\" section in the netbook so that I can train a HOI ML model based on the TSU project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70406aa5",
   "metadata": {},
   "source": [
    "## Choose dataset folder to use for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a634a8",
   "metadata": {},
   "source": [
    "R4 (Story): As a user, I can choose a dataset subfolder, using appropriate UI elements, from the data folder to use for the training so that I can select the data for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3270215",
   "metadata": {},
   "source": [
    "Description: This cell is used for setting and opening the dropdown for the user to select the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecee4e76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using Dropdown method\n",
    "path =here()\n",
    "%cd $path\n",
    "dataset_list = []\n",
    "directoryDataset = \"./data/dataset/\" # Directory of the dataset (NPY files)\n",
    "\n",
    "# Store the folder in the  dataset into dataset list\n",
    "for x in os.listdir(directoryDataset):\n",
    "    if os.path.isdir(os.path.join(directoryDataset, x)):\n",
    "        # print(os.path.join(directoryDataset, x))\n",
    "        dataset_list.append(x)\n",
    "\n",
    "datasetDropDown = widgets.Dropdown(\n",
    "    options=dataset_list,\n",
    "    value=dataset_list[0],\n",
    "    description='Dataset:')\n",
    "\n",
    "# Widgets\n",
    "datasetConfirmButton = widgets.Button(\n",
    "    description='Confirm',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "# Function on what happen when confirm is been click.\n",
    "def selectDataSet(b):\n",
    "    print(\"Selected Dataset: \" , datasetDropDown.value)\n",
    "   \n",
    "datasetConfirmButton.on_click(selectDataSet)\n",
    "datasetBox = widgets.VBox([widgets.HBox([datasetDropDown, datasetConfirmButton])])\n",
    "datasetBox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2d35e9",
   "metadata": {},
   "source": [
    "Description: This cell is used for setting and opening up the folder selector to choose the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19b18eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using Browse folder method\n",
    "def folderSelector():\n",
    "    starting_directory = './data/dataset'\n",
    "    chooser = FileChooser(starting_directory)\n",
    "    chooser.show_only_dirs = True\n",
    "    display(chooser)\n",
    "    return chooser\n",
    "\n",
    "datafolder = folderSelector()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e20405",
   "metadata": {},
   "source": [
    "## Run_PDAN.sh for training\n",
    "R4 (Story): As a user, I want to able to change the value for the argument in run_PDAN shell script with a UI so that I does not need to keep changing the value directly in the shell script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3ae2eb",
   "metadata": {},
   "source": [
    "Description: This cell is used for form to fill up the parameters needed to run the run_PDAN.sh for training. Running the run_PDAN.sh script based on the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d10fe28",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "root_path=here()\n",
    "os.chdir(root_path)\n",
    "title = widgets.Label(\"Run_PDAN\")\n",
    "style = {'description_width': '90px'}\n",
    "\n",
    "dataset_input = widgets.Text(\n",
    "    value='TSU',\n",
    "    placeholder='Enter Dataset Name',\n",
    "    description='Dataset:',\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n",
    "\n",
    "mode_input = widgets.Text(\n",
    "    value='rgb',\n",
    "    placeholder='Enter Mode',\n",
    "    description='Mode:',\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n",
    "\n",
    "split_input = widgets.Text(\n",
    "    value='CS',\n",
    "    placeholder='Enter Split Setting',\n",
    "    description='Split Setting:',\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n",
    "\n",
    "# Using Dropdown method\n",
    "model_list = []\n",
    "directoryModel = \"./pipeline/models\" # Directory of the dataset (NPY files)\n",
    "\n",
    "# Store the folder in the  dataset into dataset list\n",
    "for x in os.listdir(directoryModel):\n",
    "    if os.path.isdir(os.path.join(directoryModel, x)) == False:\n",
    "        model_list.append(x)\n",
    "        \n",
    "model_input = widgets.Dropdown(\n",
    "    options=model_list,\n",
    "    value=model_list[0],\n",
    "    description='Model:',\n",
    "    style=style\n",
    ")\n",
    "\n",
    "\n",
    "train_input = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description = 'Train',\n",
    "    disabled=False,\n",
    "    indent=True,\n",
    "    style=style\n",
    ")\n",
    "\n",
    "num_channel_input =  widgets.BoundedIntText(\n",
    "    value=512,\n",
    "    min=1,\n",
    "    max=1000,\n",
    "    step=1,\n",
    "    description='Num Channel:',\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n",
    "\n",
    "lr_input = widgets.FloatText(\n",
    "    value=0.0002,\n",
    "    description='Learning Rate:',\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n",
    "\n",
    "kernel_size_input =  widgets.BoundedIntText(\n",
    "    value=3,\n",
    "    min=1,\n",
    "    max=5,\n",
    "    step=1,\n",
    "    description='Kernel Size:',\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n",
    "\n",
    "aptype_input = widgets.Text(\n",
    "    value='map',\n",
    "    placeholder='Enter APType',\n",
    "    description='APType:',\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n",
    "\n",
    "epoch_input = widgets.BoundedIntText(\n",
    "    value=140,\n",
    "    min=1,\n",
    "    max=1000,\n",
    "    step=1,\n",
    "    description='Epoch:',\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n",
    "\n",
    "batch_size_input = widgets.Dropdown(\n",
    "    options=['1', '2', '4', '8', '16', '32', '64', '128', '256', '512', '1024'],\n",
    "    value='1',\n",
    "    description='Batch_Size:',\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n",
    "\n",
    "\n",
    "comp_info_input = widgets.Text(\n",
    "    value='TSU_CS_RGB_PDAN',\n",
    "    placeholder='Enter Compute Info',\n",
    "    description='Compute Info:',\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n",
    "\n",
    "button = widgets.Button(\n",
    "    description='Save',\n",
    "    disabled=False,\n",
    "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Save Pref',\n",
    "    icon='check', # (FontAwesome names without the `fa-` prefix)\n",
    "    style=style\n",
    ")\n",
    "\n",
    "def selectWidgetSet(b):\n",
    "    print(\"Selected Dataset: \" , dataset_input.value)\n",
    "    print(\"Selected Mode: \" , mode_input.value)\n",
    "    print(\"Selected Split Setting: \" , split_input.value)\n",
    "    print(\"Selected Model: \" , model_input.value)\n",
    "    print(\"Selected Train: \" , train_input.value)\n",
    "    print(\"Selected Num Channel: \" , num_channel_input.value)\n",
    "    print(\"Selected Learning Rate: \" , lr_input.value)\n",
    "    print(\"Selected Kernel Size: \" , kernel_size_input.value)\n",
    "    print(\"Selected APType: \", aptype_input.value)\n",
    "    print(\"Selected Epoch: \", epoch_input.value)\n",
    "    print(\"Selected Batch: \" , batch_size_input.value)\n",
    "    print(\"Selected Compute Info: \" , comp_info_input.value)\n",
    "    path =here(\"./pipeline\")\n",
    "    %cd $path\n",
    "    dataset_path = os.path.join(\"../data/dataset/\" , datasetDropDown.value)\n",
    "    print(\"Dataset Path: \", dataset_path)\n",
    "    %run -i train.py -dataset $dataset_input.value -mode $mode_input.value -split_setting $split_input.value -model $model_input.value -train $train_input.value -num_channel $num_channel_input.value -lr $lr_input.value -kernelsize $kernel_size_input.value -APtype $aptype_input.value -epoch $epoch_input.value -batch_size $batch_size_input.value -comp_info $comp_info_input.value -input_folder $dataset_path \n",
    "    %wandb ict3104-team-5/training-visualisation\n",
    "    \n",
    "display(title, dataset_input, mode_input,split_input, model_input, train_input, num_channel_input, lr_input, kernel_size_input, aptype_input, epoch_input, batch_size_input, comp_info_input ,button)\n",
    "button.on_click(selectWidgetSet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d60d71",
   "metadata": {},
   "source": [
    "# To be integrated\n",
    "R4 (Story): As a user, I can add the trained model to the list of pre-trained models that can be chosen in R3 after its training so that I can add my trained model to pre-trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ff9735",
   "metadata": {},
   "source": [
    "Description: This cell is used for setting and opening up the dropdown to allow the user to choose whether to add or remove the model from a list of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e00b06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =here(\"\")\n",
    "%cd $path\n",
    "change_list = ['Add', 'Remove']\n",
    "\n",
    "confirmChangeButton = widgets.Button(\n",
    "    description='Confirm',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "modelChangeDropDown = widgets.Dropdown(\n",
    "    options=change_list,\n",
    "    description='Add/Remove: ')\n",
    "\n",
    "def selectChangeSet(b):\n",
    "    global choice\n",
    "    choice = modelChangeDropDown.value\n",
    "    print(\"Selected: \" , modelChangeDropDown.value)\n",
    "    \n",
    "confirmChangeButton.on_click(selectChangeSet)\n",
    "display(modelChangeDropDown, confirmChangeButton)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac09381",
   "metadata": {},
   "source": [
    "Description: This cell is used for setting and opening up the filechooser to select the model to be added to or removed from the list of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af76479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addModel():\n",
    "    starting_directory = '.'\n",
    "    chooser = FileChooser(starting_directory)\n",
    "    display(chooser)\n",
    "    return chooser\n",
    "\n",
    "#pretrained_models = ['TSU_', 'NVIDIA Setp Model']\n",
    "pretrained_models = []\n",
    "\n",
    "add_model = addModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aac222",
   "metadata": {},
   "source": [
    "Description: This cell is used for adding or removing the model to/from the list of models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ea2bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selected_file(add_model):\n",
    "    if add_model.selected_filename == None:\n",
    "        return  ''\n",
    "    else:\n",
    "        from pathlib import Path\n",
    "        file_name = Path(add_model.selected_filename).stem\n",
    "        return file_name\n",
    "    \n",
    "file_name = selected_file (add_model)\n",
    "\n",
    "#models_list = ['TSU', 'NVIDIA Set Model']\n",
    "models_list = []\n",
    "\n",
    "def checkExist(filename):\n",
    "    with open('models_list.txt', 'r') as f:\n",
    "        if filename in f.read():\n",
    "            f.close()\n",
    "            return True\n",
    "        else:\n",
    "            f.close()\n",
    "            return False\n",
    "        \n",
    "def addToList(filename):\n",
    "    if checkExist(filename):\n",
    "        print(filename, \" already exist\")\n",
    "    else:\n",
    "         with open('models_list.txt', 'a') as f:\n",
    "            f.write(filename + \"\\n\")\n",
    "            \n",
    "def removeFromList(filename):\n",
    "    if checkExist(filename):\n",
    "        with open('models_list.txt', 'r') as file:\n",
    "            text = file.read()\n",
    "\n",
    "\n",
    "        # Delete text and Write\n",
    "        with open('models_list.txt', 'w') as file:\n",
    "            # Delete\n",
    "            new_text = text.replace(filename, '')\n",
    "            # Write\n",
    "            file.write(new_text)\n",
    "    else:\n",
    "        print(filename, \" does not exist\")\n",
    "    \n",
    "    \n",
    "if choice == 'Add':\n",
    "    addToList(file_name)\n",
    "elif choice == 'Remove':\n",
    "    removeFromList(file_name)\n",
    "else:\n",
    "    print('Invalid choice')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba99aa7",
   "metadata": {},
   "source": [
    "# Testing Section\n",
    "R5 (Epic): As a user, I want to have a \"Testing\" section in the notebook so that I can evaluate a trained model based on the TSU project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5bfc3b",
   "metadata": {},
   "source": [
    "R5 (Story): As a user, I want to load a pretrained model using an appropriate UI component so that I can easily choose the type of pretrained model I would like to process the data with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e87d280",
   "metadata": {},
   "source": [
    "Description: This cell is used for dropdown for the user to select the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6ef6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model\n",
    "modelList = [] \n",
    "\n",
    "# Select from the list of model in the pipeline/models folder\n",
    "for x in os.listdir(\"./pipeline/models\"): \n",
    "    modelList += [x]\n",
    "\n",
    "# Widgets\n",
    "confirmButton = widgets.Button(\n",
    "    description='Confirm',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    "    icon='check'\n",
    ")\n",
    "modelDropdown = widgets.Dropdown(\n",
    "    options=modelList,\n",
    "    value=modelList[0],\n",
    "    description='Model:')\n",
    "# Function on what happen when confirm is been click.\n",
    "def selectWidgetSet(b):\n",
    "    print(\"Selected: \" , modelDropdown.value)\n",
    "\n",
    "confirmButton.on_click(selectWidgetSet)\n",
    "modelBox = widgets.VBox([widgets.HBox([modelDropdown, confirmButton])])\n",
    "modelBox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b674a8",
   "metadata": {},
   "source": [
    "# Testing Section\n",
    "R5 (Epic): As a user, I want to have a \"Testing\" section in the notebook so that I can evaluate a trained model based on the TSU project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31541446",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =here(\"./pipeline\")\n",
    "%cd $path\n",
    "\n",
    "%run -i test.py -split CS -pkl_path \"models/\"$modelDropdown.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5143aac1",
   "metadata": {},
   "source": [
    "# NVIDIA STEP Section\n",
    "R6 (Epic): As a user, I want to able to configure the notebook using appropriate UI elements coupled with the right .py modules so that R2-5 can be performed based on another pipeline, e.g., the NVIDIA STEP pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58612b38",
   "metadata": {},
   "source": [
    "<a id='STEP'></a>\n",
    "## Nvidia Step Pipeline\n",
    "R6(Story): As a user, I want to ensure selected pipeline's dependencies are changed to ensure the right dependencies are given to the appropriate models so that the selected model will be run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdb555d",
   "metadata": {},
   "source": [
    "Description: This cell is used for changing to the working directory of the STEP Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954b4e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If run all the code on top first then need to cd back to the root directory. Therefor this cell is needed\n",
    "path =here(\"./NVIDIA-STEP-MODEL/STEP\")\n",
    "%cd $path\n",
    "# %cd ../NVIDIA-STEP-MODEL/STEP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1220d04",
   "metadata": {},
   "source": [
    "Description: This cell is used for printing the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754475a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bc07d3",
   "metadata": {},
   "source": [
    "Need \"pip install pydirectory\", Added this dependency in the \"requirements.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566bc6f4",
   "metadata": {},
   "source": [
    "Description: This cell is used for running the demo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f12f59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Else run this\n",
    "%run -i setup.py build develop\n",
    "# %run -i demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dab7806",
   "metadata": {},
   "source": [
    "Description: This cell is used for displaying the first frame of the processed video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdf4465",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='datasets/demo/frames/results/2/frame0000.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd78fe48",
   "metadata": {},
   "source": [
    "T05-128: As a User, I want pre-process the input video into frames for Nvidia STEP model, So that I can run the model successfully.\n",
    "\n",
    "T05-127 As a User, I want the output of Nvidia STEP model to be in a video format, So that I can easily view the results of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75a7bbc",
   "metadata": {},
   "source": [
    "T05-119: As a user, I want to have multiple dropdowns to select the input (1) dataset (2) video so that I can select the dataset videos easily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0728041",
   "metadata": {},
   "source": [
    "Description: This cell is used for setting and  opening up the dropdowns to allow user to select the dataset and the video input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b18281ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c5a085f0aa483a83e30550c6401be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Dataset:', options=('.ipynb_checkpoints',), value='.ipynb_checkpoints')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa91d182b334e3194eb34c31ddcd7dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Video: ', options=('1.mp4',), value='1.mp4')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8600653adf0d4c138c36e952582e37d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Confirm', icon='check', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select video\n",
    "video_list = [] \n",
    "dataset_list = [] \n",
    "directoryVideo = \"./Input/\" # Directory of the video (.mp4)\n",
    "directoryDataset = \"./datasets/demo/frames/\" # Directory of the dataset (NPY files)\n",
    "\n",
    "# Store the folder in the  dataset into dataset list\n",
    "for x in os.listdir(directoryDataset):\n",
    "    if os.path.isdir(os.path.join(directoryDataset, x)):\n",
    "        # print(os.path.join(directoryDataset, x))\n",
    "        dataset_list.append(x)\n",
    "\n",
    "# Store the video names into video list\n",
    "for x in os.listdir(directoryVideo):\n",
    "    if x.endswith(\".mp4\"):\n",
    "        video_list.append(x)\n",
    "\n",
    "datasetDropDown = widgets.Dropdown(\n",
    "    options=dataset_list,\n",
    "    description='Dataset:')\n",
    "\n",
    "videoDropDown = widgets.Dropdown(\n",
    "    options=video_list,\n",
    "    description='Video: ')\n",
    "\n",
    "# Widgets\n",
    "confirmButton = widgets.Button(\n",
    "    description='Confirm',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    "    icon='check'\n",
    ")\n",
    "# Function on what happen when confirm is been click. To be intergrated with some other code\n",
    "def selectTrimSet(b):\n",
    "    print(\"Selected Dataset: \" , datasetDropDown.value)\n",
    "    print(\"Selected Video: \" , videoDropDown.value)\n",
    "\n",
    "confirmButton.on_click(selectTrimSet)\n",
    "display(datasetDropDown, videoDropDown, confirmButton)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f1ba03",
   "metadata": {},
   "source": [
    "Description: This cell is used to caption every frame of the video. Afterwards, it will run the STEP Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c0f48f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "cap= cv2.VideoCapture('Input/' + videoDropDown.value)\n",
    "i=0\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret == False:\n",
    "        break\n",
    "    cv2.imwrite('datasets/demo/frames/' + datasetDropDown.value + '/frame'+str(i).zfill(4)+'.jpg',frame)\n",
    "    i+=1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "## Traubung code will be here\n",
    "# %run demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23d9f6d",
   "metadata": {},
   "source": [
    "Description: This cell is used for reading every frames in the results folder.It will write the captions of the frames onto the output video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9872d574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "print(datasetDropDown.value)\n",
    "img_array = []\n",
    "for filename in glob.glob('datasets/demo/results/' + datasetDropDown.value + '/*.jpg'):\n",
    "    img = cv2.imread(filename) \n",
    "    height, width, layers = img.shape\n",
    "    size = (width,height)\n",
    "    img_array.append(img)\n",
    "\n",
    "\n",
    "out = cv2.VideoWriter('Output/project.mp4',cv2.VideoWriter_fourcc(*'MP4V'), 15, size)\n",
    " \n",
    "for i in range(len(img_array)):\n",
    "    out.write(img_array[i])\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e305aa",
   "metadata": {},
   "source": [
    "## Nvidia Step Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b206852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "cap= cv2.VideoCapture('Input/' + videoDropDown.value)\n",
    "i=0\n",
    "o=900\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret == False:\n",
    "        break\n",
    "    os.makedirs('datasets/ava/frames/wEAeql4z1O0/' + str(o).zfill(5))\n",
    "    cv2.imwrite('datasets/ava/frames/wEAeql4z1O0/' + str(o).zfill(5) + '/frame'+str(i).zfill(4)+'.jpg',frame)\n",
    "    i+=1\n",
    "    o+=1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7db052",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./scripts/generate_label.py \"datasets/ava/label/ava_train_v2.1.csv\" \n",
    "%run ./scripts/generate_label.py \"datasets/ava/label/ava_val_v2.1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "220f6f70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: If you want to use fp16, please apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0\n",
      "Loading Dataset...\n",
      "train set | Datalist len:  184378\n",
      "val set | Datalist len:  57408\n",
      "Training STEP on  ava\n",
      "Building I3D model...\n",
      "Building I3D head for global branch...\n",
      "Building I3D head for global branch...\n",
      "Building I3D head for global branch...\n",
      "Building I3D head for context branch...\n",
      "Loading pretrain model from pretrained/ava_cls.pth\n",
      "NUM_CHUNKS {1: 1, 2: 1, 3: 3, 4: 3}\n",
      "NUM_SAMPLE -1\n",
      "T 3\n",
      "anchor_mode 1\n",
      "base_lr 7.5e-05\n",
      "base_net i3d\n",
      "batch_size 8\n",
      "cls_thresh [0.2, 0.35, 0.5]\n",
      "conf_thresh 0.01\n",
      "cuda True\n",
      "cycle_decay 1.0\n",
      "data_root datasets/ava/\n",
      "dataset ava\n",
      "det_lr 0.00075\n",
      "det_lr0 0.00015\n",
      "det_net two_branch\n",
      "do_crop True\n",
      "do_erase True\n",
      "do_flip True\n",
      "do_photometric True\n",
      "do_proposal_augment False\n",
      "dropout 0.3\n",
      "evaluate_topk 300\n",
      "exp_name STEP-max3-i3d-two_branch\n",
      "fc_dim 256\n",
      "fp16 False\n",
      "fps 12\n",
      "freeze_affine True\n",
      "freeze_stats True\n",
      "gamma 0.1\n",
      "id2class {1: 'bend/bow (at the waist)', 3: 'crouch/kneel', 4: 'dance', 5: 'fall down', 6: 'get up', 7: 'jump/leap', 8: 'lie/sleep', 9: 'martial art', 10: 'run/jog', 11: 'sit', 12: 'stand', 13: 'swim', 14: 'walk', 15: 'answer phone', 17: 'carry/hold (an object)', 20: 'climb (e.g., a mountain)', 22: 'close (e.g., a door, a box)', 24: 'cut', 26: 'dress/put on clothing', 27: 'drink', 28: 'drive (e.g., a car, a truck)', 29: 'eat', 30: 'enter', 34: 'hit (an object)', 36: 'lift/pick up', 37: 'listen (e.g., to music)', 38: 'open (e.g., a window, a car door)', 41: 'play musical instrument', 43: 'point to (an object)', 45: 'pull (an object)', 46: 'push (an object)', 47: 'put down', 48: 'read', 49: 'ride (e.g., a bike, a car, a horse)', 51: 'sail boat', 52: 'shoot', 54: 'smoke', 56: 'take a photo', 57: 'text on/look at a cellphone', 58: 'throw', 59: 'touch (an object)', 60: 'turn (e.g., a screwdriver)', 61: 'watch (e.g., TV)', 62: 'work on a computer', 63: 'write', 64: 'fight/hit (a person)', 65: 'give/serve (an object) to (a person)', 66: 'grab (a person)', 67: 'hand clap', 68: 'hand shake', 69: 'hand wave', 70: 'hug (a person)', 72: 'kiss (a person)', 73: 'lift (a person)', 74: 'listen to (a person)', 76: 'push (another person)', 77: 'sing to (e.g., self, a person, a group)', 78: 'take (an object) from (a person)', 79: 'talk to (e.g., self, a person, a group)', 80: 'watch (a person)'}\n",
      "image_size (400, 400)\n",
      "input_type rgb\n",
      "iou_thresh 0.5\n",
      "iterative_mode temporal\n",
      "kinetics_pretrain None\n",
      "label_dict {0: 1, 1: 3, 2: 4, 3: 5, 4: 6, 5: 7, 6: 8, 7: 9, 8: 10, 9: 11, 10: 12, 11: 13, 12: 14, 13: 15, 14: 17, 15: 20, 16: 22, 17: 24, 18: 26, 19: 27, 20: 28, 21: 29, 22: 30, 23: 34, 24: 36, 25: 37, 26: 38, 27: 41, 28: 43, 29: 45, 30: 46, 31: 47, 32: 48, 33: 49, 34: 51, 35: 52, 36: 54, 37: 56, 38: 57, 39: 58, 40: 59, 41: 60, 42: 61, 43: 62, 44: 63, 45: 64, 46: 65, 47: 66, 48: 67, 49: 68, 50: 69, 51: 70, 52: 72, 53: 73, 54: 74, 55: 76, 56: 77, 57: 78, 58: 79, 59: 80}\n",
      "lambda_cls 0.1\n",
      "lambda_neighbor 1.0\n",
      "lambda_reg 5.0\n",
      "mGPUs True\n",
      "man_seed 123\n",
      "max_epochs 14\n",
      "max_iter 3\n",
      "max_pos_num 5\n",
      "means (0, 0, 0)\n",
      "milestones [322672]\n",
      "min_ratio 0.0\n",
      "momentum 0.9\n",
      "name STEP\n",
      "neg_ratio 2\n",
      "nms_thresh 0.4\n",
      "no_context False\n",
      "num_classes 60\n",
      "num_workers 4\n",
      "optimizer adam\n",
      "pool_mode align\n",
      "pool_size 7\n",
      "pretrain_path pretrained/ava_cls.pth\n",
      "print_step 500\n",
      "proposal_path_train None\n",
      "proposal_path_val None\n",
      "reg_thresh [0.2, 0.35, 0.5]\n",
      "resume_path Auto\n",
      "sampling uniform\n",
      "save_root datasets/ava/cache/STEP-max3-i3d-two_branch/\n",
      "save_step 11465\n",
      "scale_norm 2\n",
      "scheduler cosine\n",
      "selection_nms False\n",
      "selection_sampling softmax\n",
      "selection_score score\n",
      "start_epochs 0\n",
      "start_iteration 0\n",
      "stds (1, 1, 1)\n",
      "temporal_mode predict\n",
      "topk 300\n",
      "warmup_iters 1000\n",
      "weight_decay 1e-07\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\Sisters\\anaconda3\\envs\\ict3104\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"C:\\Users\\Sisters\\anaconda3\\envs\\ict3104\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\Sisters\\anaconda3\\envs\\ict3104\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\Sisters\\source\\repos\\ict3104-team05-2022\\NVIDIA-STEP-MODEL\\STEP\\data\\ava.py\", line 316, in __getitem__\n    images = read_images(self.imgpath_rgb, videoname, fid, num=TEM_REDUCE*self.T*self.chunks, fps=self.fps)    # for i3d backbone\n  File \"C:\\Users\\Sisters\\source\\repos\\ict3104-team05-2022\\NVIDIA-STEP-MODEL\\STEP\\data\\ava.py\", line 149, in read_images\n    images.extend(_load_images(img_path, num=min(num_left, fps), fps=fps, direction='backward'))\n  File \"C:\\Users\\Sisters\\source\\repos\\ict3104-team05-2022\\NVIDIA-STEP-MODEL\\STEP\\data\\ava.py\", line 176, in _load_images\n    raise ValueError(\"Image path {} not Found\".format(path))\nValueError: Image path datasets/ava/frames/T-Fc9ctuNVI/01653/ not Found\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\source\\repos\\ict3104-team05-2022\\NVIDIA-STEP-MODEL\\STEP\\train.py:587\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_metrics\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 587\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\source\\repos\\ict3104-team05-2022\\NVIDIA-STEP-MODEL\\STEP\\train.py:230\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    227\u001b[0m     log_file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mstr\u001b[39m(nets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdet_net\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m i])\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    229\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m--> 230\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\source\\repos\\ict3104-team05-2022\\NVIDIA-STEP-MODEL\\STEP\\train.py:257\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(args, nets, optimizer, scheduler, train_dataloader, val_dataloader, log_file)\u001b[0m\n\u001b[0;32m    254\u001b[0m epoch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mceil(\u001b[38;5;28mlen\u001b[39m(train_dataloader\u001b[38;5;241m.\u001b[39mdataset) \u001b[38;5;241m/\u001b[39m args\u001b[38;5;241m.\u001b[39mbatch_size))\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m epochs \u001b[38;5;241m<\u001b[39m args\u001b[38;5;241m.\u001b[39mmax_epochs:\n\u001b[1;32m--> 257\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, (images, targets, tubes, infos) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[0;32m    259\u001b[0m         images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;66;03m# adjust learning rate\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ict3104\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 521\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    524\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    525\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ict3104\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1203\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1201\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1202\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[1;32m-> 1203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ict3104\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1229\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[0;32m   1228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[1;32m-> 1229\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ict3104\\lib\\site-packages\\torch\\_utils.py:425\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexc_type, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;66;03m# Some exceptions have first argument as non-str but explicitly\u001b[39;00m\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;66;03m# have message field\u001b[39;00m\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexc_type(message\u001b[38;5;241m=\u001b[39mmsg)\n\u001b[1;32m--> 425\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexc_type(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\Sisters\\anaconda3\\envs\\ict3104\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"C:\\Users\\Sisters\\anaconda3\\envs\\ict3104\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\Sisters\\anaconda3\\envs\\ict3104\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\Sisters\\source\\repos\\ict3104-team05-2022\\NVIDIA-STEP-MODEL\\STEP\\data\\ava.py\", line 316, in __getitem__\n    images = read_images(self.imgpath_rgb, videoname, fid, num=TEM_REDUCE*self.T*self.chunks, fps=self.fps)    # for i3d backbone\n  File \"C:\\Users\\Sisters\\source\\repos\\ict3104-team05-2022\\NVIDIA-STEP-MODEL\\STEP\\data\\ava.py\", line 149, in read_images\n    images.extend(_load_images(img_path, num=min(num_left, fps), fps=fps, direction='backward'))\n  File \"C:\\Users\\Sisters\\source\\repos\\ict3104-team05-2022\\NVIDIA-STEP-MODEL\\STEP\\data\\ava.py\", line 176, in _load_images\n    raise ValueError(\"Image path {} not Found\".format(path))\nValueError: Image path datasets/ava/frames/T-Fc9ctuNVI/01653/ not Found\n"
     ]
    }
   ],
   "source": [
    "data_root=\"datasets/ava/\"\n",
    "save_root=\"datasets/ava/cache/\"\n",
    "pretrain_path=\"pretrained/ava_cls.pth\"\n",
    "\n",
    "name=\"STEP\"\n",
    "base_net=\"i3d\"\n",
    "det_net=\"two_branch\"\n",
    "resume_path=\"Auto\"\n",
    "\n",
    "T=3\n",
    "max_iter=3    # index starts from 1\n",
    "iterative_mode=\"temporal\"\n",
    "anchor_mode=\"1\"\n",
    "temporal_mode=\"predict\"\n",
    "pool_mode=\"align\"\n",
    "pool_size=7\n",
    "\n",
    "# training schedule\n",
    "num_workers=4\n",
    "max_epochs=14\n",
    "batch_size=8\n",
    "optimizer=\"adam\"\n",
    "base_lr=7.5e-5\n",
    "det_lr0=1.5e-4\n",
    "det_lr=7.5e-4\n",
    "save_step=11465\n",
    "print_step=500\n",
    "scheduler=\"cosine\"\n",
    "milestones=\"-1\"\n",
    "warmup_iters=1000\n",
    "\n",
    "# losses\n",
    "dropout=0.3\n",
    "fc_dim=256\n",
    "lambda_reg=5\n",
    "lambda_neighbor=1\n",
    "cls_thresh=\"0.2,0.35,0.5\"\n",
    "reg_thresh=\"0.2,0.35,0.5\"\n",
    "max_pos_num=5\n",
    "neg_ratio=2\n",
    "NUM_SAMPLE=-1\n",
    "topk=300\n",
    "evaluate_topk=300\n",
    "\n",
    "# data augmentation / normalization\n",
    "scale_norm=2    # for i3d\n",
    "do_flip=\"True\"\n",
    "do_crop=\"True\"\n",
    "do_photometric=\"True\"\n",
    "do_erase=\"True\"\n",
    "freeze_affine=\"True\"\n",
    "freeze_stats=\"True\"\n",
    "\n",
    "\n",
    "%run train.py --data_root $data_root --save_root $save_root \\\n",
    "    --name $name --pretrain_path $pretrain_path --resume_path $resume_path \\\n",
    "    --base_net $base_net --det_net $det_net --max_iter $max_iter --T $T \\\n",
    "    --iterative_mode $iterative_mode --anchor_mode $anchor_mode --anchor_mode $anchor_mode --temporal_mode $temporal_mode \\\n",
    "    --pool_mode $pool_mode --pool_size $pool_size --save_step $save_step --topk $topk --evaluate_topk $evaluate_topk \\\n",
    "    --num_workers $num_workers --max_epochs $max_epochs --batch_size $batch_size --print_step $print_step \\\n",
    "    --optimizer $optimizer --base_lr $base_lr --det_lr $det_lr --det_lr0 $det_lr0 --milestones $milestones \\\n",
    "    --scale_norm $scale_norm --do_flip $do_flip --do_crop $do_crop --do_photometric $do_photometric --do_erase $do_erase \\\n",
    "    --fc_dim $fc_dim --dropout $dropout --NUM_SAMPLE $NUM_SAMPLE --scheduler $scheduler --warmup_iters $warmup_iters \\\n",
    "    --cls_thresh $cls_thresh --reg_thresh $reg_thresh --max_pos_num $max_pos_num --neg_ratio $neg_ratio \\\n",
    "    --freeze_affine $freeze_affine --freeze_stats $freeze_stats --lambda_reg $lambda_reg --lambda_neighbor $lambda_neighbor "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd34fbce",
   "metadata": {},
   "source": [
    "## Nvidia Step Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9245997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772dea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273c21ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4ebacec254d4e7a6363c1bc7ffc0d93a067b75c23c5ab80637754c15d5815670"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

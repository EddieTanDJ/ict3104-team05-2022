{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c205650",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3aebdb",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "Description: this cell is used for importing the relevant libraries for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae75fb79",
   "metadata": {
    "hide_input": false,
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true,
    "tags": [
     "trial"
    ]
   },
   "outputs": [],
   "source": [
    "# Upload File using ipyfilechooser library\n",
    "from ipyfilechooser import FileChooser\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from tqdm.notebook import tqdm, trange\n",
    "# Video Player\n",
    "from IPython.display import Video, display, clear_output\n",
    "import time \n",
    "# Get the root directory of the project\n",
    "from pyprojroot import here\n",
    "# Copy File\n",
    "import shutil\n",
    "# Widget Packages\n",
    "import ipywidgets as widgets\n",
    "# In case widget extension not working\n",
    "# jupyter nbextension enable --py widge|tsnbextension\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "# load_dotenv()\n",
    "# os.environ['WANDB_API_KEY'] = os.getenv('WANDB_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7cb0ea",
   "metadata": {},
   "source": [
    "Description: This cell is used for opening the file selector for user to select the video as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7459f1aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def videoselectorinput():\n",
    "    starting_directory = './data/video'\n",
    "    chooser = FileChooser(starting_directory)\n",
    "    display(chooser)\n",
    "    return chooser\n",
    "def videoselectoroutput():\n",
    "    starting_directory = './pipeline/video/output'\n",
    "    chooser = FileChooser(starting_directory)\n",
    "    display(chooser)\n",
    "    return chooser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd71b6b",
   "metadata": {},
   "source": [
    "## Pipeline Selection\n",
    "R6(Story): As a user, I want to create appropriate UI elements to allow for switching pipelines so that I can test the different models.\n",
    "\n",
    "### Adding new pipeline\n",
    "To add a new pipeline, add in the name of the pipeline in the options list in the dropdown widgets.\n",
    "\n",
    "T05-157 (Story): As a user, I want to run the pipeline based on the dropdown selected. So that, I can integrate the pipeline dropdown into the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7527021f",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "Description: This cell is used for setting and opening up the dropdown for the user to select the pipeline. After selecting and clicking \"Confirm\" the code will then run the selected pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "205ee99d",
   "metadata": {
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2772ea916fe54907b6a8aaaf86b5bea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Pipeline:', options=('TSU', 'STEP'), value='TSU'), Button(â€¦"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sisters\\source\\repos\\ict3104-team05-2022\\NVIDIA-STEP-MODEL\\STEP\n",
      "Running  STEP  pipeline\n",
      "Loaded dependencies\n"
     ]
    }
   ],
   "source": [
    "#print(os.getcwd())\n",
    "pipelineList = ['TSU', 'STEP']\n",
    "pipelineDropdown = widgets.Dropdown(\n",
    "    options=pipelineList,\n",
    "    value=pipelineList[0],\n",
    "    description='Pipeline:')\n",
    "# Function on what happen when confirm is been click.\n",
    "def selectPipelineSet(b):\n",
    "    if pipelineDropdown.value == \"STEP\":\n",
    "        directory = os.getcwd()\n",
    "        # Use Regex to match STEP\n",
    "        if re.search(r\"TSU\", directory):\n",
    "            %cd ..\n",
    "            path =here(\"./NVIDIA-STEP-MODEL/STEP\")\n",
    "            %cd $path\n",
    "            \n",
    "        elif re.search(r\"STEP\", directory):\n",
    "            pass\n",
    "        else:\n",
    "            path =here(\"./NVIDIA-STEP-MODEL/STEP\")\n",
    "            %cd $path\n",
    "        print(\"Running \" , pipelineDropdown.value, \" pipeline\")\n",
    "        print(\"Loaded dependencies\")\n",
    "        #print(\"Please skip to Section 6 STEP Pipeline\")\n",
    "    elif pipelineDropdown.value == \"TSU\":\n",
    "        directory = os.getcwd()\n",
    "        # Use Regex to match STEP\n",
    "        if re.search(r\"TSU\", directory):\n",
    "            %cd pass\n",
    "        elif re.search(r\"STEP\", directory):\n",
    "            %cd ..\n",
    "            path =here(\"./pipeline\")\n",
    "            %cd $path\n",
    "        else:\n",
    "            path =here(\"./pipeline\")\n",
    "            %cd $path\n",
    "        print(\"Running \" , pipelineDropdown.value, \" pipeline\")\n",
    "        print(\"Loaded dependencies\")\n",
    "        path =here(\"./pipeline\")\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "pipelineConfirm = widgets.Button(\n",
    "    description='Confirm',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "pipelineConfirm.on_click(selectPipelineSet)\n",
    "pipelineBox = widgets.VBox([widgets.HBox([pipelineDropdown, pipelineConfirm])])\n",
    "pipelineBox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b17f71",
   "metadata": {},
   "source": [
    "<a href=\"#STEP\">Click here to go to STEP section</a>\n",
    "<br>\n",
    "<a href=\"#TSU\">Click here to go to TSU section</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68b99c7",
   "metadata": {},
   "source": [
    "# Data Exploration Section\n",
    "R2 (Epic): As a user, I want a \"Data Exploration\" section in the notebook so that I can load and display video data from the TSU project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed518c9e",
   "metadata": {},
   "source": [
    "## Video Upload / Choose using ipyfilechooser\n",
    "R2 (Story): As a user, I want to upload/choose files from the data folder through an appropriate UI component (E.g. Browse files) in a notebook code cell so that I can pick and choose the video data I would like to process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46254d0",
   "metadata": {},
   "source": [
    "Description: This cell is used for setting up the file choosers to select input video and to save output video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9251f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "video = videoselectorinput()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe9761f",
   "metadata": {},
   "source": [
    "## Upload selected video to the data folder (If needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e339780e",
   "metadata": {},
   "source": [
    "Description: This cell is used for uploading the video to data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cdc476",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Upload Function\n",
    "# from pyprojroot import here\n",
    "# import shutil\n",
    "def upload(video):\n",
    "    print(video.selected)\n",
    "    source = video.selected\n",
    "    # Source path\n",
    "    # Destination path\n",
    "    destination = (here(\"./data/video\"))\n",
    "\n",
    "    # Copy file from the selected path\n",
    "    try:\n",
    "        shutil.copy(source, destination)\n",
    "        print(\"File copied successfully.\")\n",
    "\n",
    "    # If source and destination are same\n",
    "    except shutil.SameFileError:\n",
    "        print(\"Source and destination represents the same file.\")\n",
    "\n",
    "    # If destination is a directory.\n",
    "    except IsADirectoryError:\n",
    "        print(\"Destination is a directory.\")\n",
    "\n",
    "    # If there is any permission issue\n",
    "    except PermissionError:\n",
    "        print(\"Permission denied.\")\n",
    "\n",
    "    # For other errors\n",
    "    except:\n",
    "        print(\"Error occurred while copying file.\")\n",
    "\n",
    "upload(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd190ad",
   "metadata": {},
   "source": [
    "## Video Playback\n",
    "R2 (Story): As a user, I want to see video playback of the chosen video file in an output cell so that I can check if it is the right video data I would like to process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b00bf6",
   "metadata": {},
   "source": [
    "Description: This cell is used for opening the file selector for user to select video input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ce6882",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select Video\n",
    "video = videoselectorinput()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0fea43",
   "metadata": {},
   "source": [
    "Description: This cell is used for printing the details of the selected video (the full path of the video, the name of the video and the directory the video is in). This is for the user to check if it is the same video they selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0d5e2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(video.selected)\n",
    "print(video.selected_filename)\n",
    "print(video.selected_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73380e7d",
   "metadata": {},
   "source": [
    "Description: This cell is used for running the input video. This is for the user to check if it is the same video they selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09127163",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_path = video.selected\n",
    "Video(full_path, embed=True, width=540, html_attributes=\"controls muted autoplay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51191d83",
   "metadata": {},
   "source": [
    "<a id='TSU'></a>\n",
    "# Inference Section\n",
    "\n",
    "R3 (Epic): As a user, I want to have an \"Inference\" section in the notebook so that I can perform inference using a pretrained HOI ML model based on the TSU project.\n",
    "<a href='#test'>Link to test</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90b56cd",
   "metadata": {},
   "source": [
    "## Load a pretrain model\n",
    "R3 (Story): As a user, I want to load a pre-trained model using an appropriate UI component so that I can easily load the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50fb0b0",
   "metadata": {},
   "source": [
    "Description: This cell is used for setting up the dropdown list to allow the user to select the model to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263f4135",
   "metadata": {
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select model\n",
    "path =here()\n",
    "os.chdir(path)\n",
    "modelList = [] \n",
    "\n",
    "# Select from the list of model in the pipeline/models folder\n",
    "for x in os.listdir(\"./pipeline/models\"): \n",
    "    modelList += [x]\n",
    "\n",
    "# Widgets\n",
    "confirmButton = widgets.Button(\n",
    "    description='Confirm',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    "    icon='check'\n",
    ")\n",
    "modelDropdown = widgets.Dropdown(\n",
    "    options=modelList,\n",
    "    value=modelList[0],\n",
    "    description='Model:')\n",
    "# Function on what happen when confirm is been click.\n",
    "def selectWidgetSet(b):\n",
    "    print(\"Selected: \" , modelDropdown.value)\n",
    "\n",
    "confirmButton.on_click(selectWidgetSet)\n",
    "modelBox = widgets.VBox([widgets.HBox([modelDropdown, confirmButton])])\n",
    "modelBox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cad9e82",
   "metadata": {},
   "source": [
    "## Choose Input video to load into TSU Project\n",
    "R3 (Story): As a user, I want to choose an  input video files and other related input files, using an appropriate UI component, from the TSU project so that the system is able to pass the right files to the model.\n",
    "\n",
    "Take Note: You only can select video that is in **testing subset** on the smarthome_CS_51.json file to run the inference video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9054f185",
   "metadata": {},
   "source": [
    "Description: This cell is used for selecting the video input to be passed to the TSU model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a7e1db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select Video\n",
    "video = videoselectorinput()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8854da",
   "metadata": {},
   "source": [
    "<a id='test'></a>\n",
    "## Run the model \n",
    "R3 (Story): As a user, I want to see inference results in the form of an output video with captions that indicate the current detected activity in each video frame so that I am able to see the inference results clearly on the screen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0c9682",
   "metadata": {},
   "source": [
    "Description: This cell is used for running the inference.py script based on the video and model selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b5f2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = here(\"./pipeline\")\n",
    "# %cd $path\n",
    "model = modelDropdown.value\n",
    "loadmodel = './models/' + model\n",
    "videoPath = video.selected\n",
    "videoFile = video.selected_filename\n",
    "print(videoFile)\n",
    "print(videoPath)\n",
    "%run -it inference.py  -input_video_file $videoFile -model $model  -load_model $loadmodel -video_path $videoPath\n",
    "%wandb ict3104-team-5/inference-visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d326ce9c",
   "metadata": {},
   "source": [
    "## Output Video to view the inference result "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569c1dc6",
   "metadata": {},
   "source": [
    "Description: This cell is used for print the filepath of the output video. Play the output video below the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afb7c95",
   "metadata": {
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "videoFileName = video.selected_filename[:-4]\n",
    "full_path = (os.path.join(here(\"./pipeline/video/output/\"),f\"{videoFileName}_caption.mp4\"))\n",
    "print(full_path)\n",
    "Video(full_path, embed=True, width=540, html_attributes=\"controls muted autoplay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6935d29c",
   "metadata": {},
   "source": [
    "# Feature Extraction Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87c12f9",
   "metadata": {},
   "source": [
    "## Running main feature-extraction function\n",
    "Description: This cell is used for running the main feature-extraction function to extract features from videos listed in video_paths.txt to create RGB .npy files for training later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99b994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = here(\"./i3d-feature-extraction\")\n",
    "\n",
    "%cd $path\n",
    "%run -i main.py feature_type=r21d device=\"cuda:0\" on_extraction=save_numpy streams=rgb output_path=./output/RGB_TEST file_with_video_paths=./sample/test_video_paths.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c593e13",
   "metadata": {},
   "source": [
    "## Running validate_train_test.py\n",
    "Description: This cell is used for running validate_train_test.py to remove video IDs from smarthome_cs_51.json file and create an updated version called smarthome_cs_51_v2.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba5d4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ./i3d-feature-extraction && python validate_train_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67463a88",
   "metadata": {},
   "source": [
    "# Training Section\n",
    "R4 (Epic): As a user, I want to create a \"Training\" section in the netbook so that I can train a HOI ML model based on the TSU project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70406aa5",
   "metadata": {},
   "source": [
    "## Choose dataset folder to use for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a634a8",
   "metadata": {},
   "source": [
    "R4 (Story): As a user, I can choose a dataset subfolder, using appropriate UI elements, from the data folder to use for the training so that I can select the data for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3270215",
   "metadata": {},
   "source": [
    "Description: This cell is used for setting and opening the dropdown for the user to select the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecee4e76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using Dropdown method\n",
    "path =here()\n",
    "%cd $path\n",
    "dataset_list = []\n",
    "directoryDataset = \"./data/dataset/\" # Directory of the dataset (NPY files)\n",
    "\n",
    "# Store the folder in the  dataset into dataset list\n",
    "for x in os.listdir(directoryDataset):\n",
    "    if os.path.isdir(os.path.join(directoryDataset, x)):\n",
    "        # print(os.path.join(directoryDataset, x))\n",
    "        dataset_list.append(x)\n",
    "\n",
    "datasetDropDown = widgets.Dropdown(\n",
    "    options=dataset_list,\n",
    "    value=dataset_list[0],\n",
    "    description='Dataset:')\n",
    "\n",
    "# Widgets\n",
    "datasetConfirmButton = widgets.Button(\n",
    "    description='Confirm',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "# Function on what happen when confirm is been click.\n",
    "def selectDataSet(b):\n",
    "    print(\"Selected Dataset: \" , datasetDropDown.value)\n",
    "   \n",
    "datasetConfirmButton.on_click(selectDataSet)\n",
    "datasetBox = widgets.VBox([widgets.HBox([datasetDropDown, datasetConfirmButton])])\n",
    "datasetBox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2d35e9",
   "metadata": {},
   "source": [
    "Description: This cell is used for setting and opening up the folder selector to choose the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19b18eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using Browse folder method\n",
    "def folderSelector():\n",
    "    starting_directory = './data/dataset'\n",
    "    chooser = FileChooser(starting_directory)\n",
    "    chooser.show_only_dirs = True\n",
    "    display(chooser)\n",
    "    return chooser\n",
    "\n",
    "datafolder = folderSelector()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e20405",
   "metadata": {},
   "source": [
    "## Run_PDAN.sh for training\n",
    "R4 (Story): As a user, I want to able to change the value for the argument in run_PDAN shell script with a UI so that I does not need to keep changing the value directly in the shell script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3ae2eb",
   "metadata": {},
   "source": [
    "Description: This cell is used for form to fill up the parameters needed to run the run_PDAN.sh for training. Running the run_PDAN.sh script based on the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d10fe28",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "root_path=here()\n",
    "os.chdir(root_path)\n",
    "title = widgets.Label(\"Run_PDAN\")\n",
    "style = {'description_width': '90px'}\n",
    "\n",
    "dataset_input = widgets.Text(\n",
    "    value='TSU',\n",
    "    placeholder='Enter Dataset Name',\n",
    "    description='Dataset:',\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n",
    "\n",
    "mode_input = widgets.Text(\n",
    "    value='rgb',\n",
    "    placeholder='Enter Mode',\n",
    "    description='Mode:',\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n",
    "\n",
    "split_input = widgets.Text(\n",
    "    value='CS',\n",
    "    placeholder='Enter Split Setting',\n",
    "    description='Split Setting:',\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n",
    "\n",
    "# Using Dropdown method\n",
    "model_list = []\n",
    "directoryModel = \"./pipeline/models\" # Directory of the dataset (NPY files)\n",
    "\n",
    "# Store the folder in the  dataset into dataset list\n",
    "for x in os.listdir(directoryModel):\n",
    "    if os.path.isdir(os.path.join(directoryModel, x)) == False:\n",
    "        model_list.append(x)\n",
    "        \n",
    "model_input = widgets.Dropdown(\n",
    "    options=model_list,\n",
    "    value=model_list[0],\n",
    "    description='Model:',\n",
    "    style=style\n",
    ")\n",
    "\n",
    "\n",
    "train_input = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description = 'Train',\n",
    "    disabled=False,\n",
    "    indent=True,\n",
    "    style=style\n",
    ")\n",
    "\n",
    "num_channel_input =  widgets.BoundedIntText(\n",
    "    value=512,\n",
    "    min=1,\n",
    "    max=1000,\n",
    "    step=1,\n",
    "    description='Num Channel:',\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n",
    "\n",
    "lr_input = widgets.FloatText(\n",
    "    value=0.0002,\n",
    "    description='Learning Rate:',\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n",
    "\n",
    "kernel_size_input =  widgets.BoundedIntText(\n",
    "    value=3,\n",
    "    min=1,\n",
    "    max=5,\n",
    "    step=1,\n",
    "    description='Kernel Size:',\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n",
    "\n",
    "aptype_input = widgets.Text(\n",
    "    value='map',\n",
    "    placeholder='Enter APType',\n",
    "    description='APType:',\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n",
    "\n",
    "epoch_input = widgets.BoundedIntText(\n",
    "    value=140,\n",
    "    min=1,\n",
    "    max=1000,\n",
    "    step=1,\n",
    "    description='Epoch:',\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n",
    "\n",
    "batch_size_input = widgets.Dropdown(\n",
    "    options=['1', '2', '4', '8', '16', '32', '64', '128', '256', '512', '1024'],\n",
    "    value='1',\n",
    "    description='Batch_Size:',\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n",
    "\n",
    "\n",
    "comp_info_input = widgets.Text(\n",
    "    value='TSU_CS_RGB_PDAN',\n",
    "    placeholder='Enter Compute Info',\n",
    "    description='Compute Info:',\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n",
    "\n",
    "button = widgets.Button(\n",
    "    description='Save',\n",
    "    disabled=False,\n",
    "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Save Pref',\n",
    "    icon='check', # (FontAwesome names without the `fa-` prefix)\n",
    "    style=style\n",
    ")\n",
    "\n",
    "def selectWidgetSet(b):\n",
    "    print(\"Selected Dataset: \" , dataset_input.value)\n",
    "    print(\"Selected Mode: \" , mode_input.value)\n",
    "    print(\"Selected Split Setting: \" , split_input.value)\n",
    "    print(\"Selected Model: \" , model_input.value)\n",
    "    print(\"Selected Train: \" , train_input.value)\n",
    "    print(\"Selected Num Channel: \" , num_channel_input.value)\n",
    "    print(\"Selected Learning Rate: \" , lr_input.value)\n",
    "    print(\"Selected Kernel Size: \" , kernel_size_input.value)\n",
    "    print(\"Selected APType: \", aptype_input.value)\n",
    "    print(\"Selected Epoch: \", epoch_input.value)\n",
    "    print(\"Selected Batch: \" , batch_size_input.value)\n",
    "    print(\"Selected Compute Info: \" , comp_info_input.value)\n",
    "    path =here(\"./pipeline\")\n",
    "    %cd $path\n",
    "    dataset_path = os.path.join(\"../data/dataset/\" , datasetDropDown.value)\n",
    "    print(\"Dataset Path: \", dataset_path)\n",
    "    %run -i train.py -dataset $dataset_input.value -mode $mode_input.value -split_setting $split_input.value -model $model_input.value -train $train_input.value -num_channel $num_channel_input.value -lr $lr_input.value -kernelsize $kernel_size_input.value -APtype $aptype_input.value -epoch $epoch_input.value -batch_size $batch_size_input.value -comp_info $comp_info_input.value -input_folder $dataset_path \n",
    "    %wandb ict3104-team-5/training-visualisation\n",
    "    \n",
    "display(title, dataset_input, mode_input,split_input, model_input, train_input, num_channel_input, lr_input, kernel_size_input, aptype_input, epoch_input, batch_size_input, comp_info_input ,button)\n",
    "button.on_click(selectWidgetSet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d60d71",
   "metadata": {},
   "source": [
    "# To be integrated\n",
    "R4 (Story): As a user, I can add the trained model to the list of pre-trained models that can be chosen in R3 after its training so that I can add my trained model to pre-trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ff9735",
   "metadata": {},
   "source": [
    "Description: This cell is used for setting and opening up the dropdown to allow the user to choose whether to add or remove the model from a list of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e00b06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =here(\"\")\n",
    "%cd $path\n",
    "change_list = ['Add', 'Remove']\n",
    "\n",
    "confirmChangeButton = widgets.Button(\n",
    "    description='Confirm',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "modelChangeDropDown = widgets.Dropdown(\n",
    "    options=change_list,\n",
    "    description='Add/Remove: ')\n",
    "\n",
    "def selectChangeSet(b):\n",
    "    global choice\n",
    "    choice = modelChangeDropDown.value\n",
    "    print(\"Selected: \" , modelChangeDropDown.value)\n",
    "    \n",
    "confirmChangeButton.on_click(selectChangeSet)\n",
    "display(modelChangeDropDown, confirmChangeButton)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac09381",
   "metadata": {},
   "source": [
    "Description: This cell is used for setting and opening up the filechooser to select the model to be added to or removed from the list of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af76479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addModel():\n",
    "    starting_directory = '.'\n",
    "    chooser = FileChooser(starting_directory)\n",
    "    display(chooser)\n",
    "    return chooser\n",
    "\n",
    "#pretrained_models = ['TSU_', 'NVIDIA Setp Model']\n",
    "pretrained_models = []\n",
    "\n",
    "add_model = addModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aac222",
   "metadata": {},
   "source": [
    "Description: This cell is used for adding or removing the model to/from the list of models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ea2bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selected_file(add_model):\n",
    "    if add_model.selected_filename == None:\n",
    "        return  ''\n",
    "    else:\n",
    "        from pathlib import Path\n",
    "        file_name = Path(add_model.selected_filename).stem\n",
    "        return file_name\n",
    "    \n",
    "file_name = selected_file (add_model)\n",
    "\n",
    "#models_list = ['TSU', 'NVIDIA Set Model']\n",
    "models_list = []\n",
    "\n",
    "def checkExist(filename):\n",
    "    with open('models_list.txt', 'r') as f:\n",
    "        if filename in f.read():\n",
    "            f.close()\n",
    "            return True\n",
    "        else:\n",
    "            f.close()\n",
    "            return False\n",
    "        \n",
    "def addToList(filename):\n",
    "    if checkExist(filename):\n",
    "        print(filename, \" already exist\")\n",
    "    else:\n",
    "         with open('models_list.txt', 'a') as f:\n",
    "            f.write(filename + \"\\n\")\n",
    "            \n",
    "def removeFromList(filename):\n",
    "    if checkExist(filename):\n",
    "        with open('models_list.txt', 'r') as file:\n",
    "            text = file.read()\n",
    "\n",
    "\n",
    "        # Delete text and Write\n",
    "        with open('models_list.txt', 'w') as file:\n",
    "            # Delete\n",
    "            new_text = text.replace(filename, '')\n",
    "            # Write\n",
    "            file.write(new_text)\n",
    "    else:\n",
    "        print(filename, \" does not exist\")\n",
    "    \n",
    "    \n",
    "if choice == 'Add':\n",
    "    addToList(file_name)\n",
    "elif choice == 'Remove':\n",
    "    removeFromList(file_name)\n",
    "else:\n",
    "    print('Invalid choice')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba99aa7",
   "metadata": {},
   "source": [
    "# Testing Section\n",
    "R5 (Epic): As a user, I want to have a \"Testing\" section in the notebook so that I can evaluate a trained model based on the TSU project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5bfc3b",
   "metadata": {},
   "source": [
    "R5 (Story): As a user, I want to load a pretrained model using an appropriate UI component so that I can easily choose the type of pretrained model I would like to process the data with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e87d280",
   "metadata": {},
   "source": [
    "Description: This cell is used for dropdown for the user to select the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6ef6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model\n",
    "modelList = [] \n",
    "\n",
    "# Select from the list of model in the pipeline/models folder\n",
    "for x in os.listdir(\"./pipeline/models\"): \n",
    "    modelList += [x]\n",
    "\n",
    "# Widgets\n",
    "confirmButton = widgets.Button(\n",
    "    description='Confirm',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    "    icon='check'\n",
    ")\n",
    "modelDropdown = widgets.Dropdown(\n",
    "    options=modelList,\n",
    "    value=modelList[0],\n",
    "    description='Model:')\n",
    "# Function on what happen when confirm is been click.\n",
    "def selectWidgetSet(b):\n",
    "    print(\"Selected: \" , modelDropdown.value)\n",
    "\n",
    "confirmButton.on_click(selectWidgetSet)\n",
    "modelBox = widgets.VBox([widgets.HBox([modelDropdown, confirmButton])])\n",
    "modelBox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b674a8",
   "metadata": {},
   "source": [
    "# Testing Section\n",
    "R5 (Epic): As a user, I want to have a \"Testing\" section in the notebook so that I can evaluate a trained model based on the TSU project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31541446",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =here(\"./pipeline\")\n",
    "%cd $path\n",
    "\n",
    "%run -i test.py -split CS -pkl_path \"models/\"$modelDropdown.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5143aac1",
   "metadata": {},
   "source": [
    "# NVIDIA STEP Section\n",
    "R6 (Epic): As a user, I want to able to configure the notebook using appropriate UI elements coupled with the right .py modules so that R2-5 can be performed based on another pipeline, e.g., the NVIDIA STEP pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58612b38",
   "metadata": {},
   "source": [
    "<a id='STEP'></a>\n",
    "## Nvidia Step Pipeline\n",
    "R6(Story): As a user, I want to ensure selected pipeline's dependencies are changed to ensure the right dependencies are given to the appropriate models so that the selected model will be run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566bc6f4",
   "metadata": {},
   "source": [
    "Description: This cell is used for setup the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f12f59e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Else run this\n",
    "%run -i setup.py build develop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dab7806",
   "metadata": {},
   "source": [
    "Description: This cell is used for displaying the first frame of the processed video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdf4465",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='datasets/demo/frames/results/2/frame0000.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd78fe48",
   "metadata": {},
   "source": [
    "T05-128: As a User, I want pre-process the input video into frames for Nvidia STEP model, So that I can run the model successfully.\n",
    "\n",
    "T05-127 As a User, I want the output of Nvidia STEP model to be in a video format, So that I can easily view the results of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75a7bbc",
   "metadata": {},
   "source": [
    "T05-119: As a user, I want to have multiple dropdowns to select the input (1) dataset (2) video so that I can select the dataset videos easily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0728041",
   "metadata": {},
   "source": [
    "Description: This cell is used for setting and  opening up the dropdowns to allow user to select the dataset and the video input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18281ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select video\n",
    "video_list = [] \n",
    "dataset_list = [] \n",
    "directoryVideo = \"./Input/\" # Directory of the video (.mp4)\n",
    "directoryDataset = \"./datasets/demo/frames/\" # Directory of the dataset (NPY files)\n",
    "\n",
    "# Store the folder in the  dataset into dataset list\n",
    "for x in os.listdir(directoryDataset):\n",
    "    if os.path.isdir(os.path.join(directoryDataset, x)):\n",
    "        # print(os.path.join(directoryDataset, x))\n",
    "        dataset_list.append(x)\n",
    "\n",
    "# Store the video names into video list\n",
    "for x in os.listdir(directoryVideo):\n",
    "    if x.endswith(\".mp4\"):\n",
    "        video_list.append(x)\n",
    "\n",
    "datasetDropDown = widgets.Dropdown(\n",
    "    options=dataset_list,\n",
    "    description='Dataset:')\n",
    "\n",
    "videoDropDown = widgets.Dropdown(\n",
    "    options=video_list,\n",
    "    description='Video: ')\n",
    "\n",
    "# Widgets\n",
    "confirmButton = widgets.Button(\n",
    "    description='Confirm',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    "    icon='check'\n",
    ")\n",
    "# Function on what happen when confirm is been click. To be intergrated with some other code\n",
    "def selectTrimSet(b):\n",
    "    print(\"Selected Dataset: \" , datasetDropDown.value)\n",
    "    print(\"Selected Video: \" , videoDropDown.value)\n",
    "\n",
    "confirmButton.on_click(selectTrimSet)\n",
    "display(datasetDropDown, videoDropDown, confirmButton)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f1ba03",
   "metadata": {},
   "source": [
    "Description: This cell is used to caption every frame of the video. Afterwards, it will run the STEP Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c0f48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "cap= cv2.VideoCapture('Input/' + videoDropDown.value)\n",
    "i=0\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret == False:\n",
    "        break\n",
    "    cv2.imwrite('datasets/demo/frames/' + datasetDropDown.value + '/frame'+str(i).zfill(4)+'.jpg',frame)\n",
    "    i+=1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "## Traubung code will be here\n",
    "%run -i demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23d9f6d",
   "metadata": {},
   "source": [
    "Description: This cell is used for reading every frames in the results folder.It will write the captions of the frames onto the output video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9872d574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "img_array = []\n",
    "for filename in glob.glob('datasets/demo/results/' + datasetDropDown.value + '/*.jpg'):\n",
    "    img = cv2.imread(filename) \n",
    "    height, width, layers = img.shape\n",
    "    size = (width,height)\n",
    "    img_array.append(img)\n",
    "\n",
    "\n",
    "out = cv2.VideoWriter('Output/demo-result.mp4',cv2.VideoWriter_fourcc(*'MP4V'), 15, size)\n",
    " \n",
    "for i in range(len(img_array)):\n",
    "    out.write(img_array[i])\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37ee6ec",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbfd998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014eac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./scripts/generate_label.py \"datasets/ava/label/ava_train_v2.1.csv\" \n",
    "%run ./scripts/generate_label.py \"datasets/ava/label/ava_val_v2.1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de72af6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run ./scripts/extract_clips.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a30ff7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: If you want to use fp16, please apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0\n",
      "Warning: If you want to use fp16, please apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0\n",
      "Warning: If you want to use fp16, please apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0\n",
      "Warning: If you want to use fp16, please apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0\n",
      "Loading Dataset...\n",
      "train set | Datalist len:  852\n",
      "val set | Datalist len:  897\n",
      "Training STEP on  ava\n",
      "Building I3D model...\n",
      "Building I3D head for global branch...\n",
      "Building I3D head for global branch...\n",
      "Building I3D head for global branch...\n",
      "Building I3D head for context branch...\n",
      "Loading pretrain model from pretrained/ava_step.pth\n",
      "NUM_CHUNKS {1: 1, 2: 1, 3: 3, 4: 3}\n",
      "NUM_SAMPLE -1\n",
      "T 3\n",
      "anchor_mode 1\n",
      "base_lr 7.5e-05\n",
      "base_net i3d\n",
      "batch_size 1\n",
      "cls_thresh [0.2, 0.35, 0.5]\n",
      "conf_thresh 0.01\n",
      "cuda True\n",
      "cycle_decay 1.0\n",
      "data_root datasets/ava/\n",
      "dataset ava\n",
      "det_lr 0.00075\n",
      "det_lr0 0.00015\n",
      "det_net two_branch\n",
      "do_crop True\n",
      "do_erase True\n",
      "do_flip True\n",
      "do_photometric True\n",
      "do_proposal_augment False\n",
      "dropout 0.3\n",
      "evaluate_topk 300\n",
      "exp_name STEP-max3-i3d-two_branch\n",
      "fc_dim 256\n",
      "fp16 False\n",
      "fps 12\n",
      "freeze_affine True\n",
      "freeze_stats True\n",
      "gamma 0.1\n",
      "id2class {1: 'bend/bow (at the waist)', 3: 'crouch/kneel', 4: 'dance', 5: 'fall down', 6: 'get up', 7: 'jump/leap', 8: 'lie/sleep', 9: 'martial art', 10: 'run/jog', 11: 'sit', 12: 'stand', 13: 'swim', 14: 'walk', 15: 'answer phone', 17: 'carry/hold (an object)', 20: 'climb (e.g., a mountain)', 22: 'close (e.g., a door, a box)', 24: 'cut', 26: 'dress/put on clothing', 27: 'drink', 28: 'drive (e.g., a car, a truck)', 29: 'eat', 30: 'enter', 34: 'hit (an object)', 36: 'lift/pick up', 37: 'listen (e.g., to music)', 38: 'open (e.g., a window, a car door)', 41: 'play musical instrument', 43: 'point to (an object)', 45: 'pull (an object)', 46: 'push (an object)', 47: 'put down', 48: 'read', 49: 'ride (e.g., a bike, a car, a horse)', 51: 'sail boat', 52: 'shoot', 54: 'smoke', 56: 'take a photo', 57: 'text on/look at a cellphone', 58: 'throw', 59: 'touch (an object)', 60: 'turn (e.g., a screwdriver)', 61: 'watch (e.g., TV)', 62: 'work on a computer', 63: 'write', 64: 'fight/hit (a person)', 65: 'give/serve (an object) to (a person)', 66: 'grab (a person)', 67: 'hand clap', 68: 'hand shake', 69: 'hand wave', 70: 'hug (a person)', 72: 'kiss (a person)', 73: 'lift (a person)', 74: 'listen to (a person)', 76: 'push (another person)', 77: 'sing to (e.g., self, a person, a group)', 78: 'take (an object) from (a person)', 79: 'talk to (e.g., self, a person, a group)', 80: 'watch (a person)'}\n",
      "image_size (400, 400)\n",
      "input_type rgb\n",
      "iou_thresh 0.5\n",
      "iterative_mode temporal\n",
      "kinetics_pretrain None\n",
      "label_dict {0: 1, 1: 3, 2: 4, 3: 5, 4: 6, 5: 7, 6: 8, 7: 9, 8: 10, 9: 11, 10: 12, 11: 13, 12: 14, 13: 15, 14: 17, 15: 20, 16: 22, 17: 24, 18: 26, 19: 27, 20: 28, 21: 29, 22: 30, 23: 34, 24: 36, 25: 37, 26: 38, 27: 41, 28: 43, 29: 45, 30: 46, 31: 47, 32: 48, 33: 49, 34: 51, 35: 52, 36: 54, 37: 56, 38: 57, 39: 58, 40: 59, 41: 60, 42: 61, 43: 62, 44: 63, 45: 64, 46: 65, 47: 66, 48: 67, 49: 68, 50: 69, 51: 70, 52: 72, 53: 73, 54: 74, 55: 76, 56: 77, 57: 78, 58: 79, 59: 80}\n",
      "lambda_cls 0.1\n",
      "lambda_neighbor 1.0\n",
      "lambda_reg 5.0\n",
      "mGPUs True\n",
      "man_seed 123\n",
      "max_epochs 14\n",
      "max_iter 3\n",
      "max_pos_num 5\n",
      "means (0, 0, 0)\n",
      "milestones [11928]\n",
      "min_ratio 0.0\n",
      "momentum 0.9\n",
      "name STEP\n",
      "neg_ratio 2\n",
      "nms_thresh 0.4\n",
      "no_context False\n",
      "num_classes 60\n",
      "num_workers 1\n",
      "optimizer adam\n",
      "pool_mode align\n",
      "pool_size 7\n",
      "pretrain_path pretrained/ava_step.pth\n",
      "print_step 500\n",
      "proposal_path_train None\n",
      "proposal_path_val None\n",
      "reg_thresh [0.2, 0.35, 0.5]\n",
      "resume_path Auto\n",
      "sampling uniform\n",
      "save_root datasets/ava/cache/STEP-max3-i3d-two_branch/\n",
      "save_step 11465\n",
      "scale_norm 2\n",
      "scheduler cosine\n",
      "selection_nms False\n",
      "selection_sampling softmax\n",
      "selection_score score\n",
      "start_epochs 0\n",
      "start_iteration 0\n",
      "stds (1, 1, 1)\n",
      "temporal_mode predict\n",
      "topk 300\n",
      "warmup_iters 1000\n",
      "weight_decay 1e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sisters\\anaconda3\\envs\\ict3104\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14(852) Iteration 000500 lr 4.13e-04 loss-1 0.084 loss-2 0.086 loss-3 0.064 loss_global_cls 0.046 loss_local_loc 0.004 loss_neighbor_loc 0.013 Timer 612.773(612.773) GPU usage: [5667]\n",
      "Validating at  852\n",
      "100 / 897\n",
      "200 / 897\n",
      "300 / 897\n",
      "400 / 897\n",
      "500 / 897\n",
      "600 / 897\n",
      "700 / 897\n",
      "800 / 897\n",
      "{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/answer phone': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.0,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.4362047806037917,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/climb (e.g., a mountain)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/close (e.g., a door, a box)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/crouch/kneel': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/cut': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/dance': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/dress/put on clothing': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/drink': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/drive (e.g., a car, a truck)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/eat': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/enter': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/fall down': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/fight/hit (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/get up': 0.0,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/give/serve (an object) to (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/grab (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/hand clap': 0.0,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/hand shake': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/hand wave': 0.0,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/hit (an object)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/hug (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/jump/leap': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/kiss (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lift (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lift/pick up': 0.0,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen (e.g., to music)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.35542106916912497,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/martial art': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/open (e.g., a window, a car door)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/play musical instrument': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/point to (an object)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/pull (an object)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/push (an object)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/push (another person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/put down': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/read': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/run/jog': 0.25,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sail boat': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/shoot': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sing to (e.g., self, a person, a group)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.49914569540895004,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/smoke': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/stand': 0.6311924746135085,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/swim': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/take (an object) from (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/take a photo': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.9159643381647138,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/text on/look at a cellphone': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/throw': 0.0,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.0,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/turn (e.g., a screwdriver)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.4593050492321178,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.275269292586686,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (e.g., TV)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/work on a computer': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/write': nan,\n",
      "  'PascalBoxes_Precision/mAP@0.5IOU': 0.25483351331859283}\n",
      "{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/answer phone': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.01,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.48310851089275203,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/climb (e.g., a mountain)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/close (e.g., a door, a box)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/crouch/kneel': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/cut': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/dance': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/dress/put on clothing': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/drink': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/drive (e.g., a car, a truck)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/eat': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/enter': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/fall down': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/fight/hit (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/get up': 0.0,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/give/serve (an object) to (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/grab (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/hand clap': 0.0,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/hand shake': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/hand wave': 0.0,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/hit (an object)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/hug (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/jump/leap': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/kiss (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lift (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lift/pick up': 0.0,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen (e.g., to music)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.4202433240065916,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/martial art': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/open (e.g., a window, a car door)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/play musical instrument': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/point to (an object)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/pull (an object)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/push (an object)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/push (another person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/put down': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/read': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/run/jog': 0.5826551226551226,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sail boat': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/shoot': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sing to (e.g., self, a person, a group)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.5077980520971651,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/smoke': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/stand': 0.6486157541697096,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/swim': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/take (an object) from (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/take a photo': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.9230052532082063,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/text on/look at a cellphone': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/throw': 0.0,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.0,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/turn (e.g., a screwdriver)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.7938851162469455,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.2732028542159203,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (e.g., TV)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/work on a computer': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/write': nan,\n",
      "  'PascalBoxes_Precision/mAP@0.5IOU': 0.3095009324994942}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/answer phone': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.02,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.49949473930978716,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/climb (e.g., a mountain)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/close (e.g., a door, a box)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/crouch/kneel': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/cut': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/dance': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/dress/put on clothing': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/drink': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/drive (e.g., a car, a truck)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/eat': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/enter': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/fall down': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/fight/hit (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/get up': 0.0,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/give/serve (an object) to (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/grab (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/hand clap': 0.0,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/hand shake': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/hand wave': 0.0,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/hit (an object)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/hug (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/jump/leap': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/kiss (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lift (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lift/pick up': 0.0,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen (e.g., to music)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.399301867190462,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/martial art': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/open (e.g., a window, a car door)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/play musical instrument': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/point to (an object)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/pull (an object)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/push (an object)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/push (another person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/put down': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/read': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/run/jog': 0.8022727272727272,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sail boat': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/shoot': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sing to (e.g., self, a person, a group)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.48449975555021796,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/smoke': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/stand': 0.6820434677420717,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/swim': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/take (an object) from (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/take a photo': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.9283517788726694,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/text on/look at a cellphone': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/throw': 0.0,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.0,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/turn (e.g., a screwdriver)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.8210174304155158,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.3253169893630855,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (e.g., TV)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/work on a computer': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/write': nan,\n",
      "  'PascalBoxes_Precision/mAP@0.5IOU': 0.3308199170477691}\n",
      "Iter 1: MEANAP =>0.25483351331859283\n",
      "Iter 2: MEANAP =>0.3095009324994942\n",
      "Iter 3: MEANAP =>0.3308199170477691\n",
      "\n",
      "Saving current best model, iter: 852\n",
      "\n",
      "Validation TIME::: 603.173\n",
      "\n",
      "\n",
      "Epoch 2/14(852) Iteration 001000 lr 7.50e-04 loss-1 0.085 loss-2 0.081 loss-3 0.064 loss_global_cls 0.043 loss_local_loc 0.004 loss_neighbor_loc 0.013 Timer 179.831(396.302) GPU usage: [5727]\n",
      "Epoch 2/14(852) Iteration 001500 lr 7.46e-04 loss-1 0.094 loss-2 0.090 loss-3 0.068 loss_global_cls 0.037 loss_local_loc 0.006 loss_neighbor_loc 0.020 Timer 601.587(464.730) GPU usage: [5730]\n",
      "Validating at  1704\n",
      "100 / 897\n",
      "200 / 897\n",
      "300 / 897\n",
      "400 / 897\n",
      "500 / 897\n",
      "600 / 897\n",
      "700 / 897\n",
      "800 / 897\n",
      "{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/answer phone': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.039535412491364,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.41627212551611403,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/climb (e.g., a mountain)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/close (e.g., a door, a box)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/crouch/kneel': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/cut': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/dance': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/dress/put on clothing': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/drink': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/drive (e.g., a car, a truck)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/eat': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/enter': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/fall down': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/fight/hit (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/get up': 0.0,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/give/serve (an object) to (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/grab (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/hand clap': 0.0,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/hand shake': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/hand wave': 0.0,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/hit (an object)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/hug (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/jump/leap': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/kiss (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lift (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lift/pick up': 0.0,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen (e.g., to music)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.24270879094719983,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/martial art': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/open (e.g., a window, a car door)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/play musical instrument': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/point to (an object)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/pull (an object)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/push (an object)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/push (another person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/put down': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/read': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/run/jog': 0.4293956043956044,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sail boat': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/shoot': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sing to (e.g., self, a person, a group)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.4636298229774523,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/smoke': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/stand': 0.5883310358405429,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/swim': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/take (an object) from (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/take a photo': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.8661566742582267,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/text on/look at a cellphone': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/throw': 0.0,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.08268477705362423,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/turn (e.g., a screwdriver)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.24652114515685672,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.28130142499396915,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (e.g., TV)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/work on a computer': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/write': nan,\n",
      "  'PascalBoxes_Precision/mAP@0.5IOU': 0.24376912090873032}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/answer phone': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.05077017935140664,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.39848525825179787,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/climb (e.g., a mountain)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/close (e.g., a door, a box)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/crouch/kneel': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/cut': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/dance': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/dress/put on clothing': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/drink': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/drive (e.g., a car, a truck)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/eat': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/enter': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/fall down': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/fight/hit (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/get up': 0.0,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/give/serve (an object) to (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/grab (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/hand clap': 0.0,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/hand shake': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/hand wave': 0.0,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/hit (an object)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/hug (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/jump/leap': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/kiss (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lift (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lift/pick up': 0.0,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen (e.g., to music)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.3147399326210223,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/martial art': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/open (e.g., a window, a car door)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/play musical instrument': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/point to (an object)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/pull (an object)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/push (an object)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/push (another person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/put down': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/read': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/run/jog': 0.6430672268907563,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sail boat': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/shoot': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sing to (e.g., self, a person, a group)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.42906412711616637,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/smoke': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/stand': 0.6448846486134686,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/swim': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/take (an object) from (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/take a photo': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.8975163019288264,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/text on/look at a cellphone': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/throw': 0.0,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.1650904464697568,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/turn (e.g., a screwdriver)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.5158028590590844,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.2711912616281115,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (e.g., TV)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/work on a computer': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/write': nan,\n",
      "  'PascalBoxes_Precision/mAP@0.5IOU': 0.2887074827953598}\n",
      "{ 'PascalBoxes_PerformanceByCategory/AP@0.5IOU/answer phone': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/bend/bow (at the waist)': 0.04100844225614868,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/carry/hold (an object)': 0.4150757816970859,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/climb (e.g., a mountain)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/close (e.g., a door, a box)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/crouch/kneel': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/cut': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/dance': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/dress/put on clothing': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/drink': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/drive (e.g., a car, a truck)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/eat': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/enter': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/fall down': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/fight/hit (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/get up': 0.0,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/give/serve (an object) to (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/grab (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/hand clap': 0.0,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/hand shake': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/hand wave': 0.0,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/hit (an object)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/hug (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/jump/leap': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/kiss (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lie/sleep': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lift (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/lift/pick up': 0.0,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen (e.g., to music)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/listen to (a person)': 0.2712306715048415,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/martial art': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/open (e.g., a window, a car door)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/play musical instrument': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/point to (an object)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/pull (an object)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/push (an object)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/push (another person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/put down': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/read': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/ride (e.g., a bike, a car, a horse)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/run/jog': 0.825735049264461,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sail boat': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/shoot': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sing to (e.g., self, a person, a group)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/sit': 0.4160874028634652,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/smoke': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/stand': 0.6299794318721351,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/swim': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/take (an object) from (a person)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/take a photo': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/talk to (e.g., self, a person, a group)': 0.8996568136134405,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/text on/look at a cellphone': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/throw': 0.0,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/touch (an object)': 0.021333333333333333,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/turn (e.g., a screwdriver)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/walk': 0.7029042092487359,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (a person)': 0.23250491280327784,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/watch (e.g., TV)': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/work on a computer': nan,\n",
      "  'PascalBoxes_PerformanceByCategory/AP@0.5IOU/write': nan,\n",
      "  'PascalBoxes_Precision/mAP@0.5IOU': 0.29703440323046165}\n",
      "Iter 1: MEANAP =>0.24376912090873032\n",
      "Iter 2: MEANAP =>0.2887074827953598\n",
      "Iter 3: MEANAP =>0.29703440323046165\n",
      "\n",
      "\n",
      "Validation TIME::: 596.892\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/14(852) Iteration 002000 lr 7.35e-04 loss-1 0.088 loss-2 0.095 loss-3 0.085 loss_global_cls 0.037 loss_local_loc 0.010 loss_neighbor_loc 0.019 Timer 359.645(438.459) GPU usage: [5755]\n",
      "Epoch 3/14(852) Iteration 002500 lr 7.16e-04 loss-1 0.109 loss-2 0.096 loss-3 0.091 loss_global_cls 0.036 loss_local_loc 0.011 loss_neighbor_loc 0.026 Timer 603.913(471.550) GPU usage: [5753]\n",
      "Epoch 4/14(852) Iteration 003000 lr 6.90e-04 loss-1 0.075 loss-2 0.068 loss-3 0.047 loss_global_cls 0.032 loss_local_loc 0.003 loss_neighbor_loc 0.014 Timer 607.430(494.196) GPU usage: [5767]\n",
      "Epoch 5/14(852) Iteration 003500 lr 6.57e-04 loss-1 0.073 loss-2 0.080 loss-3 0.056 loss_global_cls 0.032 loss_local_loc 0.005 loss_neighbor_loc 0.019 Timer 608.006(510.455) GPU usage: [5785]\n",
      "Epoch 5/14(852) Iteration 004000 lr 6.19e-04 loss-1 0.078 loss-2 0.071 loss-3 0.048 loss_global_cls 0.032 loss_local_loc 0.003 loss_neighbor_loc 0.017 Timer 603.757(522.118) GPU usage: [5760]\n",
      "Epoch 6/14(852) Iteration 004500 lr 5.76e-04 loss-1 0.073 loss-2 0.071 loss-3 0.058 loss_global_cls 0.026 loss_local_loc 0.006 loss_neighbor_loc 0.016 Timer 608.119(531.673) GPU usage: [5783]\n"
     ]
    }
   ],
   "source": [
    "data_root=\"datasets/ava/\"\n",
    "save_root=\"datasets/ava/cache/\"\n",
    "pretrain_path=\"pretrained/ava_step.pth\"\n",
    "\n",
    "name=\"STEP\"\n",
    "base_net=\"i3d\"\n",
    "det_net=\"two_branch\"\n",
    "resume_path=\"Auto\"\n",
    "\n",
    "T=3\n",
    "max_iter=3    # index starts from 1\n",
    "iterative_mode=\"temporal\"\n",
    "anchor_mode=\"1\"\n",
    "temporal_mode=\"predict\"\n",
    "pool_mode=\"align\"\n",
    "pool_size=7\n",
    "\n",
    "# training schedule\n",
    "num_workers=1\n",
    "max_epochs=14\n",
    "batch_size=1\n",
    "optimizer=\"adam\"\n",
    "base_lr=7.5e-5\n",
    "det_lr0=1.5e-4\n",
    "det_lr=7.5e-4\n",
    "save_step=11465\n",
    "print_step=500\n",
    "scheduler=\"cosine\"\n",
    "milestones=\"-1\"\n",
    "warmup_iters=1000\n",
    "\n",
    "# losses\n",
    "dropout=0.3\n",
    "fc_dim=256\n",
    "lambda_reg=5\n",
    "lambda_neighbor=1\n",
    "cls_thresh=\"0.2,0.35,0.5\"\n",
    "reg_thresh=\"0.2,0.35,0.5\"\n",
    "max_pos_num=5\n",
    "neg_ratio=2\n",
    "NUM_SAMPLE=-1\n",
    "topk=300\n",
    "evaluate_topk=300\n",
    "\n",
    "# data augmentation / normalization\n",
    "scale_norm=2    # for i3d\n",
    "do_flip=\"True\"\n",
    "do_crop=\"True\"\n",
    "do_photometric=\"True\"\n",
    "do_erase=\"True\"\n",
    "freeze_affine=\"True\"\n",
    "freeze_stats=\"True\"\n",
    "\n",
    "\n",
    "%run train.py --data_root $data_root --save_root $save_root \\\n",
    "    --name $name --pretrain_path $pretrain_path --resume_path $resume_path \\\n",
    "    --base_net $base_net --det_net $det_net --max_iter $max_iter --T $T \\\n",
    "    --iterative_mode $iterative_mode --anchor_mode $anchor_mode --anchor_mode $anchor_mode --temporal_mode $temporal_mode \\\n",
    "    --pool_mode $pool_mode --pool_size $pool_size --save_step $save_step --topk $topk --evaluate_topk $evaluate_topk \\\n",
    "    --num_workers $num_workers --max_epochs $max_epochs --batch_size $batch_size --print_step $print_step \\\n",
    "    --optimizer $optimizer --base_lr $base_lr --det_lr $det_lr --det_lr0 $det_lr0 --milestones $milestones \\\n",
    "    --scale_norm $scale_norm --do_flip $do_flip --do_crop $do_crop --do_photometric $do_photometric --do_erase $do_erase \\\n",
    "    --fc_dim $fc_dim --dropout $dropout --NUM_SAMPLE $NUM_SAMPLE --scheduler $scheduler --warmup_iters $warmup_iters \\\n",
    "    --cls_thresh $cls_thresh --reg_thresh $reg_thresh --max_pos_num $max_pos_num --neg_ratio $neg_ratio \\\n",
    "    --freeze_affine $freeze_affine --freeze_stats $freeze_stats --lambda_reg $lambda_reg --lambda_neighbor $lambda_neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103c75ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_root=\"datasets/ava/\"\n",
    "save_root=\"datasets/ava/cache/\"\n",
    "kinetics_pretrain=\"pretrained/i3d_kinetics.pth\"\n",
    "\n",
    "name=\"Cls\"\n",
    "base_net=\"i3d\"\n",
    "det_net=\"two_branch\"\n",
    "resume_path=\"Auto\"\n",
    "\n",
    "T=9\n",
    "max_iter=1    # index starts from 1\n",
    "iterative_mode=\"spatial\"\n",
    "pool_mode=\"align\"\n",
    "pool_size=7\n",
    "\n",
    "# training schedule\n",
    "num_workers=1\n",
    "max_epochs=14\n",
    "batch_size=1\n",
    "optimizer=\"adam\"\n",
    "base_lr=5e-5\n",
    "det_lr0=1e-4\n",
    "det_lr=5e-4\n",
    "save_step=22930\n",
    "print_step=2000\n",
    "scheduler=\"cosine\"\n",
    "milestones=\"-1\"\n",
    "warmup_iters=1000\n",
    "\n",
    "# losses\n",
    "dropout=0.3\n",
    "fc_dim=256\n",
    "\n",
    "# data augmentation / normalization\n",
    "scale_norm=2    # for i3d\n",
    "do_flip=\"True\"\n",
    "do_crop=\"True\"\n",
    "do_photometric=\"True\"\n",
    "do_erase=\"True\"\n",
    "freeze_affine=\"True\"\n",
    "freeze_stats=\"True\"\n",
    "\n",
    "\n",
    "%run train_cls.py --data_root $data_root --save_root $save_root \\\n",
    "    --name $name --resume_path $resume_path --kinetics_pretrain $kinetics_pretrain \\\n",
    "    --base_net $base_net --det_net $det_net --max_iter $max_iter --T $T \\\n",
    "    --iterative_mode $iterative_mode \\\n",
    "    --pool_mode $pool_mode --pool_size $pool_size --save_step $save_step \\\n",
    "    --num_workers $num_workers --max_epochs $max_epochs --batch_size $batch_size --print_step $print_step \\\n",
    "    --optimizer $optimizer --base_lr $base_lr --det_lr $det_lr --det_lr0 $det_lr0 --milestones $milestones \\\n",
    "    --scale_norm $scale_norm --do_flip $do_flip --do_crop $do_crop --do_photometric $do_photometric --do_erase $do_erase \\\n",
    "    --fc_dim $fc_dim --dropout $dropout  --scheduler $scheduler --warmup_iters $warmup_iters \\\n",
    "    --freeze_affine $freeze_affine --freeze_stats $freeze_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2ed053",
   "metadata": {},
   "source": [
    "## STEP Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ca107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run test.py"
   ]
  }
 ],
 "metadata": {
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python [conda env:ict3104]",
   "language": "python",
   "name": "conda-env-ict3104-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4ebacec254d4e7a6363c1bc7ffc0d93a067b75c23c5ab80637754c15d5815670"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
